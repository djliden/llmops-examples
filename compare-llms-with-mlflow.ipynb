{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing LLMs with MLFlow\n",
    "This notebook is the companion to [this blog post](https://medium.com/@dliden/comparing-llms-with-mlflow-1c69553718df). It demonstrates how to use MLFlow to compare a few small (<1B Parameters) text generation models from Hugging Face, and how to compare different generation configurations for those models.\n",
    "\n",
    "For more details, read [the blog post](https://medium.com/@dliden/comparing-llms-with-mlflow-1c69553718df)\n",
    "\n",
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate torch mlflow xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "We prepare the models for use with `mlflow.evaluate()` by wrapping them in a `pyfunc` model wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    GenerationConfig,\n",
    ")\n",
    "\n",
    "\n",
    "class PyfuncTransformer(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"PyfuncTransformer is a class that extends the mlflow.pyfunc.PythonModel class\n",
    "    and is used to create a custom MLflow model for text generation using Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, gen_config_dict=None, examples=\"\"):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the PyfuncTransformer class.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained Transformer model to use.\n",
    "            gen_config_dict (dict): A dictionary of generation configuration parameters.\n",
    "            examples: examples for multi-shot prompting, prepended to the input.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.gen_config_dict = (\n",
    "            gen_config_dict if gen_config_dict is not None else {}\n",
    "        )\n",
    "        self.examples = examples\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer using the specified model_name.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            # device_map=\"auto\"\n",
    "            # make the device CPU\n",
    "            device_map=\"cpu\",\n",
    "        )\n",
    "\n",
    "        # Create a custom GenerationConfig\n",
    "        gcfg = GenerationConfig.from_model_config(model.config)\n",
    "        for key, value in self.gen_config_dict.items():\n",
    "            if hasattr(gcfg, key):\n",
    "                setattr(gcfg, key, value)\n",
    "\n",
    "        # Apply the GenerationConfig to the model's config\n",
    "        model.config.update(gcfg.to_dict())\n",
    "\n",
    "        self.model = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generates text based on the provided model_input using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for input_text in model_input:\n",
    "            output = self.model(\n",
    "                self.examples + input_text, return_full_text=False\n",
    "            )\n",
    "            generated_text.append(\n",
    "                output[0][\"generated_text\"],\n",
    "            )\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcfg = {\n",
    "    \"max_length\": 180,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "example = (\n",
    "    \"Q: Are elephants larger than mice?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Are mice carnivorous?\\nA: No, mice are typically omnivores.\\n\\n\"\n",
    "    \"Q: What is the average lifespan of an elephant?\\nA: The average lifespan of an elephant in the wild is about 60 to 70 years.\\n\\n\"\n",
    "    \"Q: Is Mount Everest the highest mountain in the world?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Which city is known as the 'City of Love'?\\nA: Paris is often referred to as the 'City of Love'.\\n\\n\"\n",
    "    \"Q: What is the capital of Australia?\\nA: The capital of Australia is Canberra.\\n\\n\"\n",
    "    \"Q: Who wrote the novel '1984'?\\nA: The novel '1984' was written by George Orwell.\\n\\n\"\n",
    ")\n",
    "\n",
    "bloom560 = PyfuncTransformer(\n",
    "    \"bigscience/bloom-560m\",\n",
    "    gen_config_dict=gcfg,\n",
    "    examples=example,\n",
    ")\n",
    "gpt2large = PyfuncTransformer(\n",
    "    \"gpt2-large\",\n",
    "    gen_config_dict=gcfg,\n",
    "    examples=example,\n",
    ")\n",
    "distilgpt2 = PyfuncTransformer(\n",
    "    \"distilgpt2\",\n",
    "    gen_config_dict=gcfg,\n",
    "    examples=example,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the Models with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"compare_small_models\")\n",
    "run_ids = []\n",
    "artifact_paths = []\n",
    "model_names = [\"bloom560\", \"gpt2large\", \"distilgpt2\"]\n",
    "\n",
    "for model, name in zip([bloom560, gpt2large, distilgpt2], model_names):\n",
    "    with mlflow.start_run(run_name=f\"log_model_{name}\"):\n",
    "        pyfunc_model = model\n",
    "        artifact_path = f\"models/{name}\"\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=artifact_path,\n",
    "            python_model=pyfunc_model,\n",
    "            input_example=\"Q: What color is the sky?\\nA:\",\n",
    "        )\n",
    "        run_ids.append(mlflow.active_run().info.run_id)\n",
    "        artifact_paths.append(artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"Q: What color is the sky?\\nA:\",\n",
    "            \"Q: Are trees plants or animals?\\nA:\",\n",
    "            \"Q: What is 2+2?\\nA:\",\n",
    "            \"Q: Who is Darth Vader?\\nA:\",\n",
    "            \"Q: What is your favorite color?\\nA:\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Models with `mlflow.evaluate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    with mlflow.start_run(\n",
    "        run_id=run_ids[i]\n",
    "    ):  # reopen the run with the stored run ID\n",
    "        evaluation_results = mlflow.evaluate(\n",
    "            model=f\"runs:/{run_ids[i]}/{artifact_paths[i]}\",\n",
    "            model_type=\"text\",\n",
    "            data=eval_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Results Table\n",
    "You can also view the results in the Artifacts view in the MLFlow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.load_table(\"eval_results_table.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Generation Parameters at Inference Time\n",
    "We can modify the approach above to accept generation configuration parameters at inference time, so we can compare many of the same inputs with different generation configurations and track those configurations in the evaluation table.\n",
    "\n",
    "## Defining the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class PyfuncTransformerWithParams(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"PyfuncTransformer is a class that extends the mlflow.pyfunc.PythonModel class\n",
    "    and is used to create a custom MLflow model for text generation using Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the PyfuncTransformer class.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained Transformer model to use.\n",
    "            examples: examples for multi-shot prompting, prepended to the input.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer using the specified model_name.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        self.model = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generates text based on the provided model_input using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.to_dict(orient=\"records\")\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for record in model_input:\n",
    "            input_text = record[\"input_text\"]\n",
    "            few_shot_examples = record[\"few_shot_examples\"]\n",
    "            config_dict = record[\"config_dict\"]\n",
    "            # Update the GenerationConfig attributes with the provided config_dict\n",
    "            gcfg = GenerationConfig.from_model_config(self.model.model.config)\n",
    "            for key, value in json.loads(config_dict).items():\n",
    "                if hasattr(gcfg, key):\n",
    "                    setattr(gcfg, key, value)\n",
    "\n",
    "            output = self.model(\n",
    "                few_shot_examples + input_text,\n",
    "                generation_config=gcfg,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "            generated_text.append(output[0][\"generated_text\"])\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples_1 = (\n",
    "    \"Q: Are elephants larger than mice?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Are mice carnivorous?\\nA: No, mice are typically omnivores.\\n\\n\"\n",
    "    \"Q: What is the average lifespan of an elephant?\\nA: The average lifespan of an elephant in the wild is about 60 to 70 years.\\n\\n\"\n",
    ")\n",
    "\n",
    "few_shot_examples_2 = (\n",
    "    \"Q: Is Mount Everest the highest mountain in the world?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Which city is known as the 'City of Love'?\\nA: Paris is often referred to as the 'City of Love'.\\n\\n\"\n",
    "    \"Q: What is the capital of Australia?\\nA: The capital of Australia is Canberra.\\n\\n\"\n",
    "    \"Q: Who wrote the novel '1984'?\\nA: The novel '1984' was written by George Orwell.\\n\\n\"\n",
    ")\n",
    "\n",
    "config_dict1 = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 10,\n",
    "    \"max_length\": 180,\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "config_dict2 = {\"do_sample\": False, \"max_length\": 180, \"max_new_tokens\": 10}\n",
    "\n",
    "few_shot_examples = [few_shot_examples_1, few_shot_examples_2]\n",
    "config_dicts = [config_dict1, config_dict2]\n",
    "\n",
    "questions = [\n",
    "    \"Q: What color is the sky?\\nA:\",\n",
    "    \"Q: Are trees plants or animals?\\nA:\",\n",
    "    \"Q: What is 2+2?\\nA:\",\n",
    "    \"Q: Who is the Darth Vader?\\nA:\",\n",
    "    \"Q: What is your favorite color?\\nA:\",\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"input_text\": questions * len(few_shot_examples),\n",
    "    \"few_shot_examples\": [\n",
    "        example for example in few_shot_examples for _ in range(len(questions))\n",
    "    ],\n",
    "    \"config_dict\": [\n",
    "        json.dumps(config)\n",
    "        for config in config_dicts\n",
    "        for _ in range(len(questions))\n",
    "    ],\n",
    "}\n",
    "\n",
    "eval_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Different Generation Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"compare_generation_params\")\n",
    "model_name = \"bloom560\"\n",
    "\n",
    "with mlflow.start_run(run_name=f\"log_model_{model_name}\"):\n",
    "    # Define an input example\n",
    "    input_example = pd.DataFrame(\n",
    "        {\n",
    "            \"input_text\": \"Q: What color is the sky?\\nA:\",\n",
    "            \"few_shot_examples\": example,  # Assuming 'example' is defined and contains your few-shot prompts\n",
    "            \"config_dict\": {},  # Assuming an empty dict for the generation parameters in this example\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define the artifact_path\n",
    "    artifact_path = f\"models/{model_name}\"\n",
    "\n",
    "    # log the data\n",
    "    eval_data = mlflow.data.from_pandas(eval_df, name=\"evaluate_configurations\")\n",
    "\n",
    "    # Log the model\n",
    "    mod = mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        python_model=bloom560_with_params,\n",
    "        input_example=input_example,\n",
    "    )\n",
    "\n",
    "    # Define the model_uri\n",
    "    model_uri = f\"runs:/{mlflow.active_run().info.run_id}/{artifact_path}\"\n",
    "\n",
    "    # Evaluate the model\n",
    "    mlflow.evaluate(model=model_uri, model_type=\"text\", data=eval_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
