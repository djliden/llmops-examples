{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing OpenAI Models to Open-Source Transformers Models with MLFLow\n",
    "\n",
    "With MLFlow, you can compare outputs from open-source LLMs to outputs from hosted proprietary models such as those from OpenAI, Anthropic, or Cohere.\n",
    "\n",
    "To run this example, make sure you have your `OPENAI_API_KEY` set in your environment or in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "assert (\n",
    "    \"OPENAI_API_KEY\" in os.environ\n",
    "), \"Please set the OPENAI_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the OpenAI Model\n",
    "To use an OpenAI model for evaluation, we need to save it in MLFlow format via `mlflow.openai.log_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "\n",
    "mlflow.set_experiment(\"compare-openai-transformers-4\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"log_model_gpt-3.5-turbo\"):\n",
    "    gpt3_5_turbo_model_info = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"gpt_3_5_turbo_model\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant\",\n",
    "            }\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've logged the model, we can load it and call it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(gpt3_5_turbo_model_info.model_uri)\n",
    "print(model.predict(\"Where would you likely find a whale?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the Transformer model\n",
    "\n",
    "We'll use the HuggingFace `gpt2-xl` model and load it as a text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "from transformers import pipeline, GenerationConfig\n",
    "\n",
    "gpt2_pipe = pipeline(\"text-generation\", model=\"gpt2-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this model usable for the sake of evaluation, we'll need to wrap it in a pyfunc model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class PyfuncGpt2(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"PyfuncTransformer is a class that extends the mlflow.pyfunc.PythonModel class\n",
    "    and is used to create a custom MLflow model for text generation using Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the PyfuncTransformer class.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained Transformer model to use.\n",
    "            gen_config_dict (dict): A dictionary of generation configuration parameters.\n",
    "            examples: examples for multi-shot prompting, prepended to the input.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer using the specified model_name.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = pipeline(\"text-generation\", model=\"gpt2-xl\",\n",
    "                              device_map=\"auto\",)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generates text based on the provided model_input using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "\n",
    "        for input_text in model_input:\n",
    "            output = self.model(\n",
    "                \"Answer the following question or instruction.\\nQuestion: \" + input_text + \"\\nAnswer: \", \n",
    "                return_full_text=False,\n",
    "                do_sample=True, \n",
    "                top_k=5,\n",
    "                temperature=0.7, \n",
    "                max_new_tokens = 15,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "            output_text = output[0][\"generated_text\"]\n",
    "            cutoff_index = output_text.find('Question: ')\n",
    "            # Cut off the text before this position if 'Question: ' is found. If not, return the full text.\n",
    "            short_output = output_text if cutoff_index == -1 else output_text[:cutoff_index]\n",
    "            generated_text.append(short_output)\n",
    "\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can log this model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = PyfuncGpt2()\n",
    "\n",
    "with mlflow.start_run(run_name=f\"log_model_gpt2\"):\n",
    "    pyfunc_model = gpt2\n",
    "    artifact_path = f\"gpt_2_xl_model\"\n",
    "    gpt2_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        python_model=pyfunc_model,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load and use this model in the same way as we loaded and used the OpenAI model. MLFlow provides a consistent API, making it straightforward to compare the two models, even though one is an open source model while the other is accessed via a proprietary API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(gpt2_model_info.model_uri)\n",
    "print(model.predict(\"Where would you likely find a whale?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Models\n",
    "\n",
    "We can use `mlflow.evaluate()` (as described in [this post](https://medium.com/@dliden/comparing-llms-with-mlflow-1c69553718df)) to compare the two models. First, we generate a comparison dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\"question\": [\"The Bering Strait is a narrow sea passage that separates Russia and Alaska. The crossing is approximately 82 kilometers wide and is named after Vitus Bering. What does the passage say is 82 kilometers wide?\",\n",
    "\"Does the following sentence express a positive or negative sentiment? The movie was enjoyable and exceeded my expectations.\",\n",
    "\"Can the hypothesis be inferred from the premise? Premise: The trophy would not fit in the brown suitcase because it was too big. Hypothesis: The trophy was too big for the suitcase.\",\n",
    "\"Where would you likely find a whale?\",\n",
    "\"What is the capital of Canada?\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [gpt3_5_turbo_model_info, gpt2_model_info]:\n",
    "    with mlflow.start_run(\n",
    "        run_id=i.run_id,\n",
    "    ):  # reopen the run with the stored run ID\n",
    "        evaluation_results = mlflow.evaluate(\n",
    "            model=f\"runs:/{i.run_id}/{i.artifact_path}\",\n",
    "            model_type=\"text\",\n",
    "            data=eval_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
