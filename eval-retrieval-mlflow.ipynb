{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b503be-93f3-4655-b62c-6a61d7522de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Evaluating RAG quality with MLFlow\n",
    "This notebook demonstrates how to use MLFlow to evaluate the quality of a Retrieval-Augmented Generation (RAG) system. We will:\n",
    "- Split, vectorize, and index a text with ChromaDB\n",
    "- Configure an MLFlow model that queries the vector DB based on a user prompt and summarizes the results\n",
    "- Compare the output to an expected output with `mlflow.evaluate`.\n",
    "\n",
    "## Setting up the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4223b8-5101-48c0-8fc6-38fe1342b1b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install einops\n",
    "!pip install chromadb\n",
    "!pip install --upgrade typing-extensions\n",
    "\n",
    "def is_databricks():\n",
    "    try:\n",
    "        dbutils\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "  \n",
    "if is_databricks():\n",
    "  dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ffe047-4aa4-47fc-8dac-bedfd0ab458f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set up chromadb collection\n",
    "import chromadb\n",
    "import openai\n",
    "import mlflow\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "docs = chroma_client.create_collection(\"retrieval_docs\", get_or_create=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ef736e-cc0b-4954-bb8b-540e0877aa32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if is_databricks():\n",
    "  os.environ[\"OPENAI_API_KEY\"]=dbutils.secrets.get(scope=\"daniel.liden\", key=\"OPENAI_API_KEY\")\n",
    "  os.environ['TRANSFORMERS_CACHE'] = \"/dbfs/daniel.liden/cache/hf/\"\n",
    "  openai.api_key=dbutils.secrets.get(scope=\"daniel.liden\", key=\"OPENAI_API_KEY\")\n",
    "else:\n",
    "  load_dotenv()\n",
    "\n",
    "assert (\n",
    "    \"OPENAI_API_KEY\" in os.environ\n",
    "), \"Please set the OPENAI_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652a6ea1-aad1-438e-a754-8eab4156504f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    \"OPENAI_API_KEY\" in os.environ\n",
    "), \"Please set the OPENAI_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f93f030-6327-46dd-a566-fbf4243bfcde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For simplicity, we'll restrict our attention to one documentâ€”the [MLFlow Concepts](https://mlflow.org/docs/latest/concepts.html) docs. Let's extract the docs and split them by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192291d5-f25c-47e9-be17-ea721b2749f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract text from https://mlflow.org/docs/latest/concepts.html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # find the header and get all text after it\n",
    "    text = \"\"\n",
    "    start_collecting = False\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.name == \"h1\" and tag.text.strip().lower() == \"concepts\":\n",
    "            start_collecting = True\n",
    "        if start_collecting:\n",
    "            text += \" \" + tag.get_text()\n",
    "    # get text\n",
    "    # text = soup.get_text()\n",
    "\n",
    "    # split into sentences\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    sentences = text.split(\".\")\n",
    "    # remove leading and trailing whitespaces\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "url = \"https://mlflow.org/docs/latest/concepts.html\"\n",
    "concepts = extract_text(url)\n",
    "\n",
    "# remove footer/navigation components\n",
    "concepts = concepts[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e822a3c-a2d7-4460-907e-106945b2d315",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can add aour texts to our ChromaDB vector database. Note that, in a production setting, it would be worthwhile to spend some more time on document formatting; e.g. grouping (or omitting) code blocks and removing strings that do not contain meaningful information.\n",
    "\n",
    "[By default](https://docs.trychroma.com/embeddings#default-all-minilm-l6-v2), ChromaDB uses the `all-MiniLM-L6-v2` model to generate embeddings from the texts; this can be changed easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7418d7d3-f998-4943-818b-6c0a0c0f55a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs.add(\n",
    "    documents=concepts,\n",
    "    ids=[f\"id_{i}\" for i in range(len(concepts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd62763-b199-4fd2-9545-1d31e41cc3cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can `peek()` at the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e495810-b29d-4905-9811-70d4d39015d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5702152-389d-431b-85b7-3b87e8eecb78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can run a sample query against this database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dea8574-2371-4994-8f60-67405fc39c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = docs.query(\n",
    "    query_texts=[\"How can an individual data scientist use MLFlow?\"]\n",
    ")\n",
    "results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d465eb-f263-4941-8fc6-8a7149995231",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configuring the MLFlow Model\n",
    "We're going to write a pyfunc wrapper around an OpenAI model. We want the model to connect to the ChromaDB collection we initialized above *without* needing to save the collection as an artifact in the MLFlow tracking system (depending on your specific needs, you may actually want to log the database as an artifact; we're opting for a lighter-weight approach here).\n",
    "\n",
    "Note that we also add a `gen_context` instance method that takes the top n results from the vector database and formats them for insertion into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9a9d10-d4eb-4a0f-be94-46addfb15480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class PyfuncWithRetrieval(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A custom MLflow model for text generation with retrieval functionality.\n",
    "\n",
    "    Extends the mlflow.pyfunc.PythonModel class and utilizes a pre-trained\n",
    "    OpenAI transformer model for text generation based on an external vector\n",
    "    database for retrieval of relevant context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_name):\n",
    "        \"\"\"\n",
    "        Initialize an instance of the PyfuncWithRetrieval class.\n",
    "\n",
    "        Args:\n",
    "            db_name (str): The name of the external vector database for context retrieval.\n",
    "        \"\"\"\n",
    "        self.db_name = db_name\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Load the MLflow context.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        self.prompt_template = \"\"\"\n",
    "You are a question answering assistant. Answer the user question below based on the provided context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the context is not relevant to the question, respond that you have no relevant information.\"\"\"\n",
    "\n",
    "    def gen_context(self, db, prompt, top_n=3):\n",
    "        \"\"\"\n",
    "        Generate context from the external database based on the input prompt.\n",
    "\n",
    "        Args:\n",
    "            db: The external vector database.\n",
    "            prompt: The input prompt for the query.\n",
    "            top_n (int, optional): Number of top results to retrieve. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            str: Retrieved context from the database.\n",
    "        \"\"\"\n",
    "        results = db.query(query_texts=prompt, n_results=top_n)\n",
    "        texts = results[\"documents\"][0]\n",
    "        texts = \"\\n-----------------------------------------------------------------------------------\\n\".join(\n",
    "            texts\n",
    "        )\n",
    "        return texts\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generate text based on the provided model input.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        chroma_client = chromadb.Client()\n",
    "        collection = chroma_client.get_collection(self.db_name)\n",
    "\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for input_text in model_input:\n",
    "            context = self.gen_context(collection, input_text, top_n=3)\n",
    "            prompt = self.prompt_template.format(\n",
    "                context=context, question=input_text\n",
    "            )\n",
    "            output = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful question-answering assistant.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            )\n",
    "            output_text = output.choices[0].message.content\n",
    "            generated_text.append(output_text)\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291daac0-32ea-42a6-809b-3dfbf6624acc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can load the model, specifying the name of the relevant chromaDBcollection as we do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38ba39c-8707-4338-9ce7-96d7462e8719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#mlflow.set_experiment(\"retrieval-eval\")\n",
    "\n",
    "\n",
    "gpt_3_5_retrieval = PyfuncWithRetrieval(db_name=\"retrieval_docs\")\n",
    "with mlflow.start_run(run_name=f\"log_model_gpt_3_5_retrieval\"):\n",
    "    pyfunc_model = gpt_3_5_retrieval\n",
    "    artifact_path = f\"gpt_3_5_retrieval_model\"\n",
    "    gpt3_5_retrieval_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        python_model=pyfunc_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0939c333-3929-4e2c-941d-65d1ae7024ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(gpt3_5_retrieval_model_info.model_uri)\n",
    "print(model.predict(\"Where would you likely find a whale?\"))\n",
    "print(model.predict(\"Who can benefit from MLFlow?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5a24b4-f97b-4872-9df4-eca146080b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluating the Retrieval-Augmented Model\n",
    "Now we can use `mlflow.evaluate()` (as described in [this post](https://medium.com/@dliden/comparing-llms-with-mlflow-1c69553718df)) to try out our retrieval system on a few different prompts.\n",
    "\n",
    "First, we set up our evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52cf8256-5d0b-40f8-9404-47ce6d9a77f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"Which MLflow component helps manage the machine learning workflow by logging parameters, metrics, and artifacts?\",\n",
    "            \"True or False: MLflow Projects allow packaging data science code in a reusable format with configuration files describing its dependencies and how to run it.\",\n",
    "            \"What syntax does the MLflow Tracking API use to reference the location of artifacts?\",\n",
    "            \"How can large organizations use MLflow Model Registry?\",\n",
    "            \"Which API allows deploying models in multiple flavors for diverse platforms like Docker and Apache Spark?\",\n",
    "            \"What is the largest country in the world by area?\",\n",
    "            \"How many legs does a spider have?\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3bd9c2-88f2-4c94-a4a3-18f4ec1aefe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lastly, we evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684bea08-1695-4a19-8aa3-695519783b4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(\n",
    "    run_id=gpt3_5_retrieval_model_info.run_id,\n",
    "):  # reopen the run with the stored run ID\n",
    "    evaluation_results = mlflow.evaluate(\n",
    "        model=f\"runs:/{gpt3_5_retrieval_model_info.run_id}/{gpt3_5_retrieval_model_info.artifact_path}\",\n",
    "        model_type=\"text\",\n",
    "        data=eval_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91353e6-b335-42cb-88dc-185fdc2deb67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.load_table(\"eval_results_table.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b070ab0e-9478-44a2-884f-726aadc6df31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Compare Multiple Models\n",
    "Now we will compare the OpenAI model defined above to `falcon-7b-instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc81711-1455-49dd-9fc3-309245ad6b69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18eb6d9a-372b-4847-95fc-e389f31500de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up the pyfunc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9369cf-3d9c-4dbb-af13-00cdd1f88fb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PyfuncFalconInstr7bWithRetrieval(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"PyfuncTransformer is a class that extends the mlflow.pyfunc.PythonModel class\n",
    "    and is used to create a custom MLflow model for text generation using Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_name):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the PyfuncTransformer class.\n",
    "        \"\"\"\n",
    "        self.db_name = db_name\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer using the specified model_name.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        self.model = pipeline(\n",
    "            \"text-generation\", model=model, tokenizer=tokenizer\n",
    "        )\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        # see https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1#6478508e9c1f42c1f4d8b0bf\n",
    "        self.prompt_template = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "{question}\\n\\n\"\"\"\n",
    "\n",
    "    def gen_context(self, db, prompt, top_n=3):\n",
    "        \"\"\"\n",
    "        Generate context from the external database based on the input prompt.\n",
    "\n",
    "        Args:\n",
    "            db: The external vector database.\n",
    "            prompt: The input prompt for the query.\n",
    "            top_n (int, optional): Number of top results to retrieve. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            str: Retrieved context from the database.\n",
    "        \"\"\"\n",
    "        results = db.query(query_texts=prompt, n_results=top_n)\n",
    "        texts = results[\"documents\"][0]\n",
    "        texts = \"\\n-----------------------------------------------------------------------------------\\n\".join(\n",
    "            texts\n",
    "        )\n",
    "        return texts\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generates text based on the provided model_input using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "\n",
    "        chroma_client = chromadb.Client()\n",
    "        collection = chroma_client.get_collection(self.db_name)\n",
    "\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for input_text in model_input:\n",
    "            context = self.gen_context(collection, input_text, top_n=3)\n",
    "            prompt = self.prompt_template.format(context=context, question=input_text)\n",
    "            output = self.model(\n",
    "                prompt,\n",
    "                return_full_text=False,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "            )\n",
    "            output_text = output[0][\"generated_text\"]\n",
    "            cutoff_index = output_text.find(\"\\n\\n\")\n",
    "            # Cut off the text before this position if '\\n' is found. If not, return the full text.\n",
    "            short_output = (\n",
    "                output_text if cutoff_index == -1 else output_text[:cutoff_index]\n",
    "            )\n",
    "            generated_text.append(short_output)\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c239c1d7-ab24-442f-ae89-d3407d974bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model\n",
    "falcon_7b_retrieval = PyfuncFalconInstr7bWithRetrieval(db_name=\"retrieval_docs\")\n",
    "with mlflow.start_run(run_name=f\"log_model_falcon_7b_retrieval\"):\n",
    "    pyfunc_model = falcon_7b_retrieval\n",
    "    artifact_path = f\"falcon_7b_retrieval_model\"\n",
    "    falcon_7b_retrieval_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        python_model=pyfunc_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4373ea1d-deb2-4cc4-88f8-c611b5e17d45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(falcon_7b_retrieval_model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa657a08-ad08-495e-bf68-efa730ddbf18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print some test outputs\n",
    "print(model.predict(\"Where do whales live?\"))\n",
    "print(model.predict(\"Who can benefit from MLFlow?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d7880f-934f-4a29-b4be-26317ba6a7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run the comparison\n",
    "Note that we do not need to re-run the gpt-3.5-turbo comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06042960-53a1-43a2-9dfb-c1fcb6d34cc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(\n",
    "    run_id=falcon_7b_retrieval_model_info.run_id,\n",
    "):  # reopen the run with the stored run ID\n",
    "    evaluation_results = mlflow.evaluate(\n",
    "        model=f\"runs:/{falcon_7b_retrieval_model_info.run_id}/{falcon_7b_retrieval_model_info.artifact_path}\",\n",
    "        model_type=\"text\",\n",
    "        data=eval_df,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "eval-retrieval-mlflow",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
