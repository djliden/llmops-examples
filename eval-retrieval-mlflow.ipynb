{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG quality with MLFlow\n",
    "This notebook demonstrates how to use MLFlow to evaluate the quality of a Retrieval-Augmented Generation (RAG) system. We will:\n",
    "- Split, vectorize, and index a text with ChromaDB\n",
    "- Configure an MLFlow model that queries the vector DB based on a user prompt and summarizes the results\n",
    "- Compare the output to an expected output with `mlflow.evaluate`.\n",
    "\n",
    "## Setting up the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up chromadb collection\n",
    "import chromadb\n",
    "import openai\n",
    "import mlflow\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "assert (\n",
    "    \"OPENAI_API_KEY\" in os.environ\n",
    "), \"Please set the OPENAI_API_KEY environment variable.\"\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "docs = chroma_client.create_collection(\"retrieval_docs\", get_or_create=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll restrict our attention to one documentâ€”the [MLFlow Concepts](https://mlflow.org/docs/latest/concepts.html) docs. Let's extract the docs and split them by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from https://mlflow.org/docs/latest/concepts.html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # find the header and get all text after it\n",
    "    text = \"\"\n",
    "    start_collecting = False\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.name == \"h1\" and tag.text.strip().lower() == \"concepts\":\n",
    "            start_collecting = True\n",
    "        if start_collecting:\n",
    "            text += \" \" + tag.get_text()\n",
    "    # get text\n",
    "    # text = soup.get_text()\n",
    "\n",
    "    # split into sentences\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    sentences = text.split(\".\")\n",
    "    # remove leading and trailing whitespaces\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "url = \"https://mlflow.org/docs/latest/concepts.html\"\n",
    "concepts = extract_text(url)\n",
    "\n",
    "# remove footer/navigation components\n",
    "concepts = concepts[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add aour texts to our ChromaDB vector database. Note that, in a production setting, it would be worthwhile to spend some more time on document formatting; e.g. grouping (or omitting) code blocks and removing strings that do not contain meaningful information.\n",
    "\n",
    "[By default](https://docs.trychroma.com/embeddings#default-all-minilm-l6-v2), ChromaDB uses the `all-MiniLM-L6-v2` model to generate embeddings from the texts; this can be changed easily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.add(\n",
    "    documents=concepts,\n",
    "    ids=[f\"id_{i}\" for i in range(len(concepts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can `peek()` at the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run a sample query against this database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = docs.query(\n",
    "    query_texts=[\"How can an individual data scientist use MLFlow?\"]\n",
    ")\n",
    "results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the MLFlow Model\n",
    "We're going to write a pyfunc wrapper around an OpenAI model. We want the model to connect to the ChromaDB collection we initialized above *without* needing to save the collection as an artifact in the MLFlow tracking system (depending on your specific needs, you may actually want to log the database as an artifact; we're opting for a lighter-weight approach here).\n",
    "\n",
    "Note that we also add a `gen_context` instance method that takes the top n results from the vector database and formats them for insertion into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class PyfuncWithRetrieval(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    A custom MLflow model for text generation with retrieval functionality.\n",
    "\n",
    "    Extends the mlflow.pyfunc.PythonModel class and utilizes a pre-trained\n",
    "    OpenAI transformer model for text generation based on an external vector\n",
    "    database for retrieval of relevant context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_name):\n",
    "        \"\"\"\n",
    "        Initialize an instance of the PyfuncWithRetrieval class.\n",
    "\n",
    "        Args:\n",
    "            db_name (str): The name of the external vector database for context retrieval.\n",
    "        \"\"\"\n",
    "        self.db_name = db_name\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Load the MLflow context.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        self.prompt_template = \"\"\"\n",
    "You are a question answering assistant. Answer the user question below based on the provided context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the context is not relevant to the question, respond that you have no relevant information.\"\"\"\n",
    "\n",
    "    def gen_context(self, db, prompt, top_n=3):\n",
    "        \"\"\"\n",
    "        Generate context from the external database based on the input prompt.\n",
    "\n",
    "        Args:\n",
    "            db: The external vector database.\n",
    "            prompt: The input prompt for the query.\n",
    "            top_n (int, optional): Number of top results to retrieve. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            str: Retrieved context from the database.\n",
    "        \"\"\"\n",
    "        results = db.query(query_texts=prompt, n_results=top_n)\n",
    "        texts = results[\"documents\"][0]\n",
    "        texts = \"\\n-----------------------------------------------------------------------------------\\n\".join(\n",
    "            texts\n",
    "        )\n",
    "        return texts\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generate text based on the provided model input.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        chroma_client = chromadb.Client()\n",
    "        collection = chroma_client.get_collection(self.db_name)\n",
    "\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for input_text in model_input:\n",
    "            context = self.gen_context(collection, input_text, top_n=3)\n",
    "            prompt = self.prompt_template.format(\n",
    "                context=context, question=input_text\n",
    "            )\n",
    "            output = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful question-answering assistant.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            )\n",
    "            output_text = output.choices[0].message.content\n",
    "            generated_text.append(output_text)\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the model, specifying the name of the relevant chromaDBcollection as we do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"retrieval-eval\")\n",
    "\n",
    "\n",
    "gpt_3_5_retrieval = PyfuncWithRetrieval(db_name=\"retrieval_docs\")\n",
    "with mlflow.start_run(run_name=f\"log_model_gpt_3_5_retrieval\"):\n",
    "    pyfunc_model = gpt_3_5_retrieval\n",
    "    artifact_path = f\"gpt_3_5_retrieval_model\"\n",
    "    gpt3_5_retrieval_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        python_model=pyfunc_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(gpt3_5_retrieval_model_info.model_uri)\n",
    "print(model.predict(\"Where would you likely find a whale?\"))\n",
    "print(model.predict(\"Who can benefit from MLFlow?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Retrieval-Augmented Model\n",
    "Now we can use `mlflow.evaluate()` (as described in [this post](https://medium.com/@dliden/comparing-llms-with-mlflow-1c69553718df)) to try out our retrieval system on a few different prompts.\n",
    "\n",
    "First, we set up our evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"Which MLflow component helps manage the machine learning workflow by logging parameters, metrics, and artifacts?\",\n",
    "            \"True or False: MLflow Projects allow packaging data science code in a reusable format with configuration files describing its dependencies and how to run it.\",\n",
    "            \"What syntax does the MLflow Tracking API use to reference the location of artifacts?\",\n",
    "            \"How can large organizations use MLflow Model Registry?\",\n",
    "            \"Which API allows deploying models in multiple flavors for diverse platforms like Docker and Apache Spark?\",\n",
    "            \"What is the largest country in the world by area?\",\n",
    "            \"How many legs does a spider have?\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(\n",
    "    run_id=gpt3_5_retrieval_model_info.run_id,\n",
    "):  # reopen the run with the stored run ID\n",
    "    evaluation_results = mlflow.evaluate(\n",
    "        model=f\"runs:/{gpt3_5_retrieval_model_info.run_id}/{gpt3_5_retrieval_model_info.artifact_path}\",\n",
    "        model_type=\"text\",\n",
    "        data=eval_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.load_table(\"eval_results_table.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitsandbytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'"
     ]
    }
   ],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
