{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Spark dataframes and Delta tables as Hugging Face datasets\n",
    "\n",
    "This example notebook demonstrates how to create Hugging Face datasets from Spark dataframes and Delta tables.\n",
    "\n",
    "## Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"huggingface-from-spark\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "As demonstrated [here](databricks.com/blog/contributing-spark-loader-for-hugging-face-datasets) and [here](https://huggingface.co/docs/datasets/use_with_spark), creating a Hugging Face Dataset from a Spark dataframe or from a Delta table loaded into a DataFrame is very straightforward: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Spark DataFrame\n",
    "df = spark.createDataFrame([(1, \"Hello\"), (2, \"World\")])\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do to load the Spark DataFrame to a Hugging Face Dataset is call the `Dataset.from_spark()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_spark(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Table example\n",
    "Let's try something more substantial. We'll download the full MNIST dataset, convert it to a Delta table, and then load it as a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    BinaryType,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "train_set = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "test_set = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "# Convert the data to a Spark DataFrame\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"label\", FloatType(), False),\n",
    "        StructField(\n",
    "            \"features\", BinaryType(), False\n",
    "        ),  # Changed ArrayType(IntegerType()) to BinaryType()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Convert images to numpy arrays and save as binary\n",
    "train_data = [\n",
    "    (i, float(y), bytearray(np.array(x))) for i, (x, y) in enumerate(train_set)\n",
    "]\n",
    "train_df = spark.createDataFrame(train_data, schema).repartition(50)\n",
    "\n",
    "test_data = [\n",
    "    (i, float(y), bytearray(np.array(x))) for i, (x, y) in enumerate(test_set)\n",
    "]\n",
    "test_df = spark.createDataFrame(test_data, schema).repartition(50)\n",
    "\n",
    "# Write the DataFrame to Delta Lake format\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").save(\"./data/mnist_delta/train\")\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").save(\"./data/mnist_delta/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load the Delta Tables as DataFrames and use them to create a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.format(\"delta\").load(\"./data/mnist_delta/train\")\n",
    "test_df = spark.read.format(\"delta\").load(\"./data/mnist_delta/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta tables as Spark DataFrames\n",
    "train_df = spark.read.format(\"delta\").load(\"./data/mnist_delta/train\")\n",
    "test_df = spark.read.format(\"delta\").load(\"./data/mnist_delta/test\")\n",
    "\n",
    "# Create Hugging Face Datasets from the Spark DataFrames\n",
    "train_dataset = Dataset.from_spark(train_df)\n",
    "test_dataset = Dataset.from_spark(test_df)\n",
    "\n",
    "# Concatenate the train and test datasets into a single dataset\n",
    "huggingface_dataset = dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we can reclaim our original data after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "row = train_dataset[8]\n",
    "# Extract the image data and label\n",
    "image_data = row[\"features\"]\n",
    "label = row[\"label\"]\n",
    "\n",
    "# Convert the binary data back to a NumPy array and reshape it\n",
    "image_array = np.frombuffer(image_data, dtype=np.uint8).reshape(28, 28)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_array, cmap=\"gray\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-340-delta-240",
   "language": "python",
   "name": "pyspark-340-delta-240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
