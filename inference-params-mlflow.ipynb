{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Parameters in MLFlow 2.6\n",
    "\n",
    "MLFlow 2.6 [added the ability](https://github.com/mlflow/mlflow/releases/tag/v2.6.0) to pass inference parameters to PyFunc models at inference time. This makes it easier to, for example, adjust sampling parametes such as `temperarure` and `top_k` or adjust the number of tokens to be returned with `max_new_tokens` in Hugging Face models at inference time without building those parameters into the input data.\n",
    "\n",
    "Here's how it's done.\n",
    "\n",
    "## Define the Pyfunc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import transformers\n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        # Load the pre-trained model\n",
    "        model = transformers.pipeline(\n",
    "            task=\"text-generation\", model=\"gpt2\", eos_token_id=50256\n",
    "        )\n",
    "\n",
    "        # Generate text using the provided parameters\n",
    "        generated_text = []\n",
    "        for t in model_input[\"input\"]:\n",
    "            generated_text.append(model(t, **params))\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "my_model = MyModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model signature\n",
    "To use inferenc parameters, we need to define a model signature that includes a `params` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\"input\": [\"The sky is\", \"The ocean is\"]}\n",
    "parameters = {\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 5,\n",
    "    \"temperature\": 0.,\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "\n",
    "signature = mlflow.models.infer_signature(model_input=data, params=parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=my_model,\n",
    "        signature=signature,\n",
    "        artifact_path=\"my-model\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and run the logged model\n",
    "In the cell below, we load the logged model and run it twice, once with default inference parameters and once with the parameters set explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the logged mlflow model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "data = {\"input\": [\"The sky is\", \"The ocean is\"]}\n",
    "\n",
    "# Example with no parameters\n",
    "result_no_params = loaded_model.predict(data, params={})\n",
    "\n",
    "# Example with do_sample=true and temperature=2\n",
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 4.,\n",
    "    \"top_k\": 25,\n",
    "    \"max_new_tokens\": 100\n",
    "}\n",
    "result_with_params = loaded_model.predict(data, params=params)\n",
    "\n",
    "print(\"Default params:\")\n",
    "for response in result_no_params:\n",
    "    print(\"Prompt 1:\", response[0][0]['generated_text'])\n",
    "    print(\"Prompt 2:\", response[1][0]['generated_text'])\n",
    "\n",
    "print(\"\\nExplicit params:\")\n",
    "for response in result_with_params:\n",
    "    print(\"Prompt 1:\", response[0][0]['generated_text'])\n",
    "    print(\"Prompt 2:\", response[1][0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
