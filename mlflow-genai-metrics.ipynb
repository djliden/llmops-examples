{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Evaluation Metrics in MLflow\n",
    "\n",
    "MLflow 2.8 introduced new [Generative AI Metrics](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#generative-ai-metrics) that use LLMs to evaluate model output text. There are a few different GenAI metrics to choose from:\n",
    "- [Answer Correctness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which compares a model's output to a ground truth answer\n",
    "- [Answer Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which evaluates how appropriate and applicable a response is with respect to the input\n",
    "- [Answer Similarity](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity), which assesses the semantic similarity of a generated response to a ground truth answer\n",
    "- [Faithfulness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness), which tests the factual similarity of a model's response to some provided context (e.g. in a RAG system)\n",
    "- [Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.relevance), which examines the output with respect to the input and provided context (e.g. in a RAG system) and rates its relevance and significance. Note that this differs from the `Answer Similarity` metric, which does not have a context component.\n",
    "\n",
    "These all work in fudamentally the same way: pick a model and (optionally) define an example, at which point you can use the new metric in the MLflow.evaluate() system. Let's go through a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_correctness, greater_is_better=True, long_name=answer_correctness, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_correctness based on the input and output.\n",
      "A definition of answer_correctness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer correctness is evaluated on the accuracy of the provided output based on the provided targets, which is the ground truth. Scores can be assigned based on the degree of semantic similarity and factual correctness of the provided output to the provided targets, where a higher score indicates higher degree of accuracy.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Correctness: Below are the details for different scores:\n",
      "- Score 1: The output is completely incorrect. It is completely different from or contradicts the provided targets.\n",
      "- Score 2: The output demonstrates some degree of semantic similarity and includes partially correct information. However, the output still has significant discrepancies with the provided targets or inaccuracies.\n",
      "- Score 3: The output addresses a couple of aspects of the input accurately, aligning with the provided targets. However, there are still omissions or minor inaccuracies.\n",
      "- Score 4: The output is mostly correct. It provides mostly accurate information, but there may be one or more minor omissions or inaccuracies.\n",
      "- Score 5: The output is correct. It demonstrates a high degree of accuracy and semantic similarity to the targets.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "What is MLflow Tracking?\n",
      "\n",
      "Output:\n",
      "MLflow Tracking's API and UI log ML workflow aspects like parameters, code, metrics, and artifacts. It provides a unified view of a model's development, aiding team analysis. Designed for diverse environments like scripts or notebooks, it simplifies result logging to files or servers, enhancing run comparisons for users.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time. Whether working in standalone scripts, notebooks, or other environments, Tracking facilitates the logging of results either to local files or a server, making it easier to compare multiple runs across different users.\n",
      "\n",
      "score: 5\n",
      "justification: The answer gives a correct summary of MLflow tracking. The answer does not include any innacuracies or significant omissions.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is the MLflow Model Registry?\n",
      "\n",
      "Output:\n",
      "MLflow's Model Registry is a version control hub for managing ML model versions. It helps track model stages and facilitates a smooth transition to production with a centralized model store and UI.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "A systematic approach to model management, the Model Registry assists in handling different versions of models, discerning their current state, and ensuring a smooth transition from development to production. It offers a centralized model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, including model lineage, versioning, stage transitions, and annotations.\n",
      "\n",
      "score: 3\n",
      "justification: The output inaccurately characterizes the Model Registry as a version control only system, thereby failing to capture its broader role in the full lifecycle management of machine learning models. This overlooks key features such as collaboration, annotations, and comprehensive lifecycle management, leading to a deduction in the correctness score.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_correctness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_correctness score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, answer_correctness\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow Tracking?\",\n",
    "    output=\"MLflow Tracking's API and UI log ML workflow aspects like parameters, \"\n",
    "    \"code, metrics, and artifacts. It provides a unified view of a model's \"\n",
    "    \"development, aiding team analysis. Designed for diverse environments \"\n",
    "    \"like scripts or notebooks, it simplifies result logging to files or servers, \"\n",
    "    \"enhancing run comparisons for users.\",\n",
    "    score=5,\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "        \"of parameters, code versions, metrics, and artifacts during the ML process. This \"\n",
    "        \"centralized repository captures details such as parameters, metrics, artifacts, \"\n",
    "        \"data, and environment configurations, giving teams insight into their models’ \"\n",
    "        \"evolution over time. Whether working in standalone scripts, notebooks, or other \"\n",
    "        \"environments, Tracking facilitates the logging of results either to local files or a \"\n",
    "        \"server, making it easier to compare multiple runs across different users.\"\n",
    "    },\n",
    "    justification=\"The answer gives a correct summary of MLflow tracking. \"\n",
    "    \"The answer does not include any innacuracies or significant omissions.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is the MLflow Model Registry?\",\n",
    "    output=\"MLflow's Model Registry is a version control hub for managing \"\n",
    "    \"ML model versions. It helps track model stages and facilitates a \"\n",
    "    \"smooth transition to production with a centralized model store and UI.\",\n",
    "    score=3,\n",
    "    grading_context={\n",
    "        \"targets\": \"A systematic approach to model management, the Model Registry assists \"\n",
    "        \"in handling different versions of models, discerning their current state, and \"\n",
    "        \"ensuring a smooth transition from development to production. It offers a centralized \"\n",
    "        \"model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, \"\n",
    "        \"including model lineage, versioning, stage transitions, and annotations.\"\n",
    "    },\n",
    "    justification=\"The output inaccurately characterizes the Model Registry as a version control \"\n",
    "    \"only system, thereby failing to capture its broader role in the full lifecycle management \"\n",
    "    \"of machine learning models. This overlooks key features such as collaboration, annotations, \"\n",
    "    \"and comprehensive lifecycle management, leading to a deduction in the correctness score.\",\n",
    ")\n",
    "\n",
    "example3 = EvaluationExample(\n",
    "    input=\"What is automatic logging in MLflow?\",\n",
    "    output=\"Automatic logging in MLflow is an AI-driven feature for optimizing data storage, \"\n",
    "    \"leveraging algorithms to enhance data retrieval and backups within the ML workflow.\",\n",
    "    score=1,\n",
    "    grading_context={\n",
    "        \"targets\": \"Automatic logging allows you to log metrics, parameters, and models \"\n",
    "        \"without the need for explicit log statements. There are two ways to use autologging: Call \"\n",
    "        \"mlflow.autolog() before your training code. This will enable autologging for each supported \"\n",
    "        \"library you have installed as soon as you import it. Use library-specific autolog calls for \"\n",
    "        \"each library you use in your code.\"\n",
    "    },\n",
    "    justification=\"The output erroneously represents automatic logging as a data storage optimization \"\n",
    "    \"feature, which is entirely incorrect. Automatic logging in MLflow is actually designed to log \"\n",
    "    \"metrics, parameters, and models automatically during the machine learning model training process. \"\n",
    "    \"This significant misstatement of MLflow's functionality warrants the score of 1.\",\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_correctness_metric = answer_correctness(\n",
    "    model=\"openai:/gpt-4\", examples=[example1, example2]\n",
    ")\n",
    "\n",
    "print(answer_correctness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open source platform for managing the end-to-end machine learning lifecycle.\",\n",
    "            \"Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\",\n",
    "            \"To exit vim, press ESC to enter command mode, then type :q and press Enter.\",\n",
    "            \"To exit emacs, press Ctrl+x, then Ctrl+c.\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 14:56:31 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/06 14:56:31 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/06 14:56:36 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29189bea8f84f8397f56289c23cd52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 14:56:47 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/06 14:56:47 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/06 14:56:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/06 14:56:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/06 14:56:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/06 14:56:48 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fc82ffafbb4e98899e2f189049bee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.00042040476910187863,\n",
       " 'toxicity/v1/variance': 8.061732311598358e-08,\n",
       " 'toxicity/v1/p90': 0.0007380339549854398,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 10.4,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 11.745,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 13.84,\n",
       " 'ari_grade_level/v1/mean': 11.375,\n",
       " 'ari_grade_level/v1/variance': 20.481875,\n",
       " 'ari_grade_level/v1/p90': 16.11,\n",
       " 'exact_match/v1': 0.0,\n",
       " 'answer_correctness/v1/mean': 4.5,\n",
       " 'answer_correctness/v1/variance': 0.25,\n",
       " 'answer_correctness/v1/p90': 5.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question.\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "        targets=\"ground_truth\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness_metric,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d4d6d0c9944d1197dfcd6b1554d6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ec031\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ec031_level0_col0\" class=\"col_heading level0 col0\" >inputs</th>\n",
       "      <th id=\"T_ec031_level0_col1\" class=\"col_heading level0 col1\" >outputs</th>\n",
       "      <th id=\"T_ec031_level0_col2\" class=\"col_heading level0 col2\" >answer_correctness/v1/score</th>\n",
       "      <th id=\"T_ec031_level0_col3\" class=\"col_heading level0 col3\" >answer_correctness/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ec031_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ec031_row0_col0\" class=\"data row0 col0\" >What is MLflow?</td>\n",
       "      <td id=\"T_ec031_row0_col1\" class=\"data row0 col1\" >MLflow is an open-source platform used for managing the machine learning lifecycle. It provides tracking of experiments, packaging and reproducibility of models, and deployment of models to different execution environments.</td>\n",
       "      <td id=\"T_ec031_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_ec031_row0_col3\" class=\"data row0 col3\" >The output accurately describes MLflow as an open-source platform used for managing the machine learning lifecycle, including tracking of experiments, packaging and reproducibility of models, and deployment of models to different execution environments. This aligns closely with the provided targets, demonstrating a high degree of semantic similarity and factual correctness. There are no significant omissions or inaccuracies in the output. Therefore, the answer_correctness score is 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec031_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ec031_row1_col0\" class=\"data row1 col0\" >What is Delta Lake?</td>\n",
       "      <td id=\"T_ec031_row1_col1\" class=\"data row1 col1\" >Delta Lake is an open-source storage layer that brings reliability and scalability to data lakes. It provides ACID transactions, schema enforcement, and metadata management capabilities for big data workloads.</td>\n",
       "      <td id=\"T_ec031_row1_col2\" class=\"data row1 col2\" >4</td>\n",
       "      <td id=\"T_ec031_row1_col3\" class=\"data row1 col3\" >The output correctly identifies Delta Lake as an open-source storage layer that brings reliability and scalability to data lakes. It also correctly mentions that it provides ACID transactions, schema enforcement, and metadata management capabilities for big data workloads. However, it omits the specific mention of Apache Spark, which is included in the target information. This omission is minor, but it does prevent the output from fully aligning with the target information. Therefore, the answer is mostly correct, but with a minor omission.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec031_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ec031_row2_col0\" class=\"data row2 col0\" >How to exit vim?</td>\n",
       "      <td id=\"T_ec031_row2_col1\" class=\"data row2 col1\" >To exit vim, you can press the Esc key to enter command mode, and then type :q! to quit without saving or :wq to save changes and quit. Press Enter to execute the command.</td>\n",
       "      <td id=\"T_ec031_row2_col2\" class=\"data row2 col2\" >4</td>\n",
       "      <td id=\"T_ec031_row2_col3\" class=\"data row2 col3\" >The output is mostly correct and aligns with the provided targets. It correctly explains how to exit vim by pressing the Esc key to enter command mode and then typing :q! to quit without saving or :wq to save changes and quit. However, the output includes an additional command (:q!) that is not mentioned in the targets. This is a minor discrepancy, but it does not significantly affect the overall correctness of the output. Therefore, the output receives a score of 4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec031_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ec031_row3_col0\" class=\"data row3 col0\" >How to exit emacs?</td>\n",
       "      <td id=\"T_ec031_row3_col1\" class=\"data row3 col1\" >To exit Emacs, you can press the Ctrl and x keys together, followed by the Ctrl and c keys together.</td>\n",
       "      <td id=\"T_ec031_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_ec031_row3_col3\" class=\"data row3 col3\" >The output provided by the model is completely accurate and matches the target information. The model correctly states that to exit Emacs, one needs to press Ctrl+x, then Ctrl+c. There are no inaccuracies or omissions in the model's output.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2c9e2ba50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "df[\n",
    "    [\n",
    "        \"inputs\",\n",
    "        \"outputs\",\n",
    "        \"answer_correctness/v1/score\",\n",
    "        \"answer_correctness/v1/justification\",\n",
    "    ]\n",
    "].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_relevance, greater_is_better=True, long_name=answer_relevance, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_relevance based on the input and output.\n",
      "A definition of answer_relevance and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer relevance measures the appropriateness and applicability of the output with respect to the input. Scores should reflect the extent to which the output directly addresses the question provided in the input, and give lower scores for incomplete or redundant output.\n",
      "\n",
      "Grading rubric:\n",
      "Answer relevance: Please give a score from 1-5 based on the degree of relevance to the input, where the lowest and highest scores are defined as follows:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the input.\n",
      "- Score 5: The output addresses all aspects of the question and all parts of the output are meaningful and relevant to the question.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "What is MLflow Tracking?\n",
      "\n",
      "Output:\n",
      "MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time.\n",
      "\n",
      "\n",
      "\n",
      "score: 5\n",
      "justification: The answer directly addresses the input question and  provides a concise and clear description of MLflow Tracking.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is MLflow Model Registry?\n",
      "\n",
      "Output:\n",
      "MLflow Model Registry is a component of MLflow that helps in managing and deploying models in production. It provides versioning and stage transitions. MLflow also has a model evaluation feature for evaluating ML models.\n",
      "\n",
      "\n",
      "\n",
      "score: 3\n",
      "justification: The answer provides a general idea about MLflow Model Registry and includes correct details about versioning and stage transitions. The mention of model evaluation is irrelevant to the input question, hence the score of 3.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is automatic logging in MLflow?\n",
      "\n",
      "Output:\n",
      "Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\n",
      "\n",
      "\n",
      "\n",
      "score: 1\n",
      "justification: The output is completely irrelevant to the input question about automatic logging in MLflow, hence the score of 1.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_relevance based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_relevance score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import answer_relevance\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow Tracking?\",\n",
    "    output=\"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "    \"of parameters, code versions, metrics, and artifacts during the ML process. \"\n",
    "    \"This centralized repository captures details such as parameters, metrics, \"\n",
    "    \"artifacts, data, and environment configurations, giving teams insight into their \"\n",
    "    \"models’ evolution over time.\",\n",
    "    score=5,\n",
    "    justification=\"The answer directly addresses the input question and  \"\n",
    "    \"provides a concise and clear description of MLflow Tracking.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is MLflow Model Registry?\",\n",
    "    output=\"MLflow Model Registry is a component of MLflow that helps in managing \"\n",
    "    \"and deploying models in production. It provides versioning and stage transitions. \"\n",
    "    \"MLflow also has a model evaluation feature for evaluating ML models.\",\n",
    "    score=3,\n",
    "    justification=\"The answer provides a general idea about MLflow Model Registry and \"\n",
    "    \"includes correct details about versioning and stage transitions. The mention of model evaluation \"\n",
    "    \"is irrelevant to the input question, hence the score of 3.\",\n",
    ")\n",
    "\n",
    "example3 = EvaluationExample(\n",
    "    input=\"What is automatic logging in MLflow?\",\n",
    "    output=\"Delta Lake is an open-source storage layer that brings ACID transactions to Apache \"\n",
    "    \"Spark and big data workloads.\",\n",
    "    score=1,\n",
    "    justification=\"The output is completely irrelevant to the input question about \"\n",
    "    \"automatic logging in MLflow, hence the score of 1.\",\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_relevance_metric = answer_relevance(\n",
    "    model=\"openai:/gpt-4\", examples=[example1, example2, example3]\n",
    ")\n",
    "\n",
    "print(answer_relevance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2023/11/06 14:56:59 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/06 14:56:59 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/06 14:57:13 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69f5487d4664ca2bffa21127a1d6885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 14:57:17 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/06 14:57:17 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/06 14:57:17 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/06 14:57:17 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/06 14:57:17 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/06 14:57:17 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_relevance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dd402a32a74699b3c642ccb68f4051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.0003918855873052962,\n",
       " 'toxicity/v1/variance': 6.071496609846221e-08,\n",
       " 'toxicity/v1/p90': 0.0006513843894936144,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 8.55,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 20.232499999999998,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 13.350000000000001,\n",
       " 'ari_grade_level/v1/mean': 10.0,\n",
       " 'ari_grade_level/v1/variance': 39.995000000000005,\n",
       " 'ari_grade_level/v1/p90': 16.66,\n",
       " 'exact_match/v1': 0.0,\n",
       " 'answer_relevance/v1/mean': 5.0,\n",
       " 'answer_relevance/v1/variance': 0.0,\n",
       " 'answer_relevance/v1/p90': 5.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question.\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "        targets=\"ground_truth\",\n",
    "        extra_metrics=[\n",
    "         answer_relevance_metric,\n",
    "        ],  # use the answer similarity metric created above\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466d5bf8d2b14b77a0028ca69c0a373c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3a881\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3a881_level0_col0\" class=\"col_heading level0 col0\" >inputs</th>\n",
       "      <th id=\"T_3a881_level0_col1\" class=\"col_heading level0 col1\" >outputs</th>\n",
       "      <th id=\"T_3a881_level0_col2\" class=\"col_heading level0 col2\" >answer_relevance/v1/score</th>\n",
       "      <th id=\"T_3a881_level0_col3\" class=\"col_heading level0 col3\" >answer_relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3a881_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3a881_row0_col0\" class=\"data row0 col0\" >What is MLflow?</td>\n",
       "      <td id=\"T_3a881_row0_col1\" class=\"data row0 col1\" >MLflow is an open-source platform used for managing the complete machine learning lifecycle.</td>\n",
       "      <td id=\"T_3a881_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_3a881_row0_col3\" class=\"data row0 col3\" >The output directly addresses the input question and provides a concise and clear description of MLflow. It is completely relevant to the question asked, hence the score of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a881_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3a881_row1_col0\" class=\"data row1 col0\" >What is Delta Lake?</td>\n",
       "      <td id=\"T_3a881_row1_col1\" class=\"data row1 col1\" >Delta Lake is an open-source storage layer that provides reliability, scalability, and performance improvements to data lakes. It allows for ACID transactions, schema enforcement, and data versioning, making it easier to build robust and scalable data pipelines for analytics and machine learning.</td>\n",
       "      <td id=\"T_3a881_row1_col2\" class=\"data row1 col2\" >5</td>\n",
       "      <td id=\"T_3a881_row1_col3\" class=\"data row1 col3\" >The output provides a comprehensive and accurate answer to the input question about Delta Lake. It mentions that Delta Lake is an open-source storage layer and goes on to describe its key features and benefits, such as reliability, scalability, performance improvements, ACID transactions, schema enforcement, and data versioning. All parts of the output are meaningful and relevant to the question, hence the score of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a881_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3a881_row2_col0\" class=\"data row2 col0\" >How to exit vim?</td>\n",
       "      <td id=\"T_3a881_row2_col1\" class=\"data row2 col1\" >To exit vim, you can use the command :q or :wq to save and quit, or :q! to force quit without saving.</td>\n",
       "      <td id=\"T_3a881_row2_col2\" class=\"data row2 col2\" >5</td>\n",
       "      <td id=\"T_3a881_row2_col3\" class=\"data row2 col3\" >The output directly answers the input question by providing the commands to exit vim. It also provides additional relevant information about how to save and quit, or force quit without saving, which are all relevant to the question. Therefore, the answer is complete and directly relevant to the question, hence the score of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a881_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3a881_row3_col0\" class=\"data row3 col0\" >How to exit emacs?</td>\n",
       "      <td id=\"T_3a881_row3_col1\" class=\"data row3 col1\" >To exit Emacs, you can use the command C-x C-c.</td>\n",
       "      <td id=\"T_3a881_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_3a881_row3_col3\" class=\"data row3 col3\" >The output directly answers the input question by providing the exact command to exit Emacs. All parts of the output are meaningful and relevant to the question, hence the score of 5.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2e384bd50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "\n",
    "df[[\"inputs\", \"outputs\", \"answer_relevance/v1/score\", \"answer_relevance/v1/justification\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "The [*faithfulness*](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness) metric assesses \"how factually consistent the output is to the context.\" So for this metric, we require input, output, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called faithfulness based on the input and output.\n",
      "A definition of faithfulness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n",
      "\n",
      "Grading rubric:\n",
      "Faithfulness: Below are the details for different scores:\n",
      "- Score 1: None of the claims in the output can be inferred from the provided context.\n",
      "- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n",
      "- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n",
      "- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n",
      "- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "What is MLflow Model Registry?\n",
      "\n",
      "Output:\n",
      "The Model Registry centralizes MLflow Models in a repository, offering APIs, a user interface, and features like versioning, state tracking, and annotations, as well as the ability to archive, delete, and search models for a seamless transition from development to production and ongoing management.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "A systematic approach to model management, the Model Registry assists in handling different versions of models, discerning their current state, and ensuring a smooth transition from development to production. It offers a centralized model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, including model lineage, versioning, stage transitions, and annotations.\n",
      "\n",
      "score: 3\n",
      "justification: The output receives a score of 3 rather than a lower score like 1 because it does accurately reflect most of the core details mentioned in the original context such as centralized storage, APIs, UI, versioning, state tracking, and annotations. However, it introduces extraneous details like the ability to 'archive, delete, and search models,' which are not derived from the context. Additionally, it substitutes 'repository' for 'store' and introduces 'ongoing management,' slight deviations from the original description. These inaccuracies and additions are not fundamentally wrong, but they are not faithful to the original context, warranting a deduction in score.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's faithfulness score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"What is MLflow Tracking?\",\n",
    "        output=\"MLflow Tracking offers an API and UI for centralized logging of machine \"\n",
    "        \"learning parameters, code versions, metrics, and artifacts, supporting a variety \"\n",
    "        \"of environments and enabling easier comparison of multiple runs across users.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a clear answer including only details from the provided context\",\n",
    "        grading_context={\n",
    "            \"context\": \"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "            \"of parameters, code versions, metrics, and artifacts during the ML process. This \"\n",
    "            \"centralized repository captures details such as parameters, metrics, artifacts, \"\n",
    "            \"data, and environment configurations, giving teams insight into their models’ \"\n",
    "            \"evolution over time. Whether working in standalone scripts, notebooks, or other \"\n",
    "            \"environments, Tracking facilitates the logging of results either to local files or a \"\n",
    "            \"server, making it easier to compare multiple runs across different users.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is MLflow Model Registry?\",\n",
    "        output=\"The Model Registry centralizes MLflow Models in a repository, offering APIs, a \"\n",
    "        \"user interface, and features like versioning, state tracking, and annotations, as \"\n",
    "        \"well as the ability to archive, delete, and search models for a seamless transition \"\n",
    "        \"from development to production and ongoing management.\",\n",
    "        score=3,\n",
    "        justification=\"The output receives a score of 3 rather than a lower score like 1 \"\n",
    "        \"because it does accurately reflect most of the core details mentioned in the \"\n",
    "        \"original context such as centralized storage, APIs, UI, versioning, state tracking, \"\n",
    "        \"and annotations. However, it introduces extraneous details like the ability to \"\n",
    "        \"'archive, delete, and search models,' which are not derived from the context. \"\n",
    "        \"Additionally, it substitutes 'repository' for 'store' and introduces 'ongoing \"\n",
    "        \"management,' slight deviations from the original description. These inaccuracies \"\n",
    "        \"and additions are not fundamentally wrong, but they are not faithful to the original \"\n",
    "        \"context, warranting a deduction in score.\",\n",
    "        grading_context={\n",
    "            \"context\": \"A systematic approach to model management, the Model Registry assists \"\n",
    "            \"in handling different versions of models, discerning their current state, and \"\n",
    "            \"ensuring a smooth transition from development to production. It offers a centralized \"\n",
    "            \"model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, \"\n",
    "            \"including model lineage, versioning, stage transitions, and annotations.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is automatic logging in MLflow?\",\n",
    "        output=\"Automatic logging in MLflow is a machine learning technique used to automate the \"\n",
    "        \"process of cutting down trees for lumber.\",\n",
    "        score=1,\n",
    "        justification=\"The output is entirely incorrect and fails to utilize the given context. \"\n",
    "        \"The context is about MLflow's automatic logging for metrics, parameters, and models in \"\n",
    "        \"machine learning projects. Instead, the output discusses using machine learning to automate \"\n",
    "        \"the process of cutting down trees, which is completely unrelated to the actual context provided.\",\n",
    "        grading_context={\n",
    "            \"context\": \"Automatic logging allows you to log metrics, parameters, and models without the \"\n",
    "            \"need for explicit log statements. There are two ways to use autologging: Call mlflow.autolog() \"\n",
    "            \"before your training code. This will enable autologging for each supported library you have \"\n",
    "            \"installed as soon as you import it. Use library-specific autolog calls for each library you use \"\n",
    "            \"in your code.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=\"openai:/gpt-4\", examples=faithfulness_examples[1:2]\n",
    ")\n",
    "print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"context\": [\n",
    "            \"MLflow, at its core, provides a suite of tools aimed at simplifying the ML \"\n",
    "            \"workflow. It is tailored to assist ML practitioners throughout the various \"\n",
    "            \"stages of ML development and deployment. Despite its expansive offerings, \"\n",
    "            \"MLflow’s functionalities are rooted in several foundational components:\"\n",
    "            \"Tracking: MLflow Tracking provides both an API and UI dedicated to the \"\n",
    "            \"logging of parameters, code versions, metrics, and artifacts during the ML \"\n",
    "            \"process. This centralized repository captures details such as parameters, \"\n",
    "            \"metrics, artifacts, data, and environment configurations, giving teams \"\n",
    "            \"insight into their models’ evolution over time. Whether working in standalone \"\n",
    "            \"scripts, notebooks, or other environments, Tracking facilitates the logging \"\n",
    "            \"of results either to local files or a server, making it easier to compare \"\n",
    "            \"multiple runs across different users.\"\n",
    "            \"Model Registry: A systematic approach to model management, the Model Registry \"\n",
    "            \"assists in handling different versions of models, discerning their current \"\n",
    "            \"state, and ensuring a smooth transition from development to production. It \"\n",
    "            \"offers a centralized model store, APIs, and UI to collaboratively manage an \"\n",
    "            \"MLflow Model’s full lifecycle, including model lineage, versioning, stage \"\n",
    "            \"transitions, and annotations.\"\n",
    "            \"AI Gateway: This server, equipped with a set of standardized APIs, streamlines \"\n",
    "            \"access to both SaaS and OSS LLM models. It serves as a unified interface, \"\n",
    "            \"bolstering security through authenticated access, and offers a common set of \"\n",
    "            \"APIs for prominent LLMs.\"\n",
    "            \"Evaluate: Designed for in-depth model analysis, this set of tools facilitates \"\n",
    "            \"objective model comparison, be it traditional ML algorithms or cutting-edge \"\n",
    "            \"LLMs.\"\n",
    "            \"Prompt Engineering UI: A dedicated environment for prompt engineering, this \"\n",
    "            \"UI-centric component provides a space for prompt experimentation, refinement, \"\n",
    "            \"evaluation, testing, and deployment.\"\n",
    "            \"Recipes: Serving as a guide for structuring ML projects, Recipes, while \"\n",
    "            \"offering recommendations, are focused on ensuring functional end results \"\n",
    "            \"optimized for real-world deployment scenarios.\"\n",
    "            \"Projects: MLflow Projects standardize the packaging of ML code, workflows, \"\n",
    "            \"and artifacts, akin to an executable. Each project, be it a directory with \"\n",
    "            \"code or a Git repository, employs a descriptor or convention to define its \"\n",
    "            \"dependencies and execution method.\",\n",
    "            \"Delta Lake is an open source project that enables building a Lakehouse \"\n",
    "            \"architecture on top of data lakes. Delta Lake provides ACID transactions, \"\n",
    "            \"scalable metadata handling, and unifies streaming and batch data processing \"\n",
    "            \"on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\"\n",
    "            \"Specifically, Delta Lake offers:\"\n",
    "            \"ACID transactions on Spark: Serializable isolation levels ensure that readers \"\n",
    "            \"never see inconsistent data.\"\n",
    "            \"Scalable metadata handling: Leverages Spark distributed processing power to \"\n",
    "            \"handle all the metadata for petabyte-scale tables with billions of files at ease.\"\n",
    "            \"Streaming and batch unification: A table in Delta Lake is a batch table as well \"\n",
    "            \"as a streaming source and sink. Streaming data ingest, batch historic backfill, \"\n",
    "            \"interactive queries all just work out of the box.\"\n",
    "            \"Schema enforcement: Automatically handles schema variations to prevent insertion \"\n",
    "            \"of bad records during ingestion.\"\n",
    "            \"Time travel: Data versioning enables rollbacks, full historical audit trails, \"\n",
    "            \"and reproducible machine learning experiments.\"\n",
    "            \"Upserts and deletes: Supports merge, update and delete operations to enable \"\n",
    "            \"complex use cases like change-data-capture, slowly-changing-dimension (SCD) \"\n",
    "            \"operations, streaming upserts, and so on.\",\n",
    "            \"After saving your changes, you can quit Vim with :q. Or the saving and \"\n",
    "            \"quitting can be combined into one operation with :wq or :x.\"\n",
    "            \"If you want to discard any changes, enter :q! to quit Vim without saving.\",\n",
    "            \"C-x C-c\\n\\n    Kill Emacs (save-buffers-kill-terminal).\"\n",
    "            \"C-z\\n\\n    On a text terminal, suspend Emacs; on a graphical display, \"\n",
    "            \"iconify (or “minimize”) the selected frame (suspend-frame).\"\n",
    "            \"Killing Emacs means terminating the Emacs program. To do this, type \"\n",
    "            \"C-x C-c (save-buffers-kill-terminal). A two-character key sequence is \"\n",
    "            \"used to make it harder to type by accident. If there are any modified \"\n",
    "            \"file-visiting buffers when you type C-x C-c, Emacs first offers to save \"\n",
    "            \"these buffers. If you do not save them all, it asks for confirmation \"\n",
    "            \"again, since the unsaved changes will be lost. Emacs also asks for \"\n",
    "            \"confirmation if any subprocesses are still running, since killing \"\n",
    "            \"Emacs will also kill the subprocesses (see Running Shell Commands from Emacs).\"\n",
    "            \"C-x C-c behaves specially if you are using Emacs as a server. If you \"\n",
    "            \"type it from a client frame, it closes the client connection. See Using \"\n",
    "            \"Emacs as a Server.\",\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2023/11/06 14:59:46 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/06 14:59:46 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/06 15:00:13 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caeb2fb9eb240b1976766bb6fc11db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 15:00:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/06 15:00:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/06 15:00:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/06 15:00:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/06 15:00:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/06 15:00:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3b47705a8943798e2feceafdfda9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.0005084216718387324,\n",
       " 'toxicity/v1/variance': 1.2682792350723286e-07,\n",
       " 'toxicity/v1/p90': 0.0008852531376760453,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 13.525,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 147.90187500000002,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 26.720000000000006,\n",
       " 'ari_grade_level/v1/mean': 17.724999999999998,\n",
       " 'ari_grade_level/v1/variance': 207.89687500000002,\n",
       " 'ari_grade_level/v1/p90': 33.290000000000006,\n",
       " 'faithfulness/v1/mean': 5.0,\n",
       " 'faithfulness/v1/variance': 0.0,\n",
       " 'faithfulness/v1/p90': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question using only the information provided in the context.\"\n",
    "    basic_context_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_context_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        predictions=\"result\",\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"question\",\n",
    "                \"context\": \"context\"\n",
    "            }\n",
    "        },\n",
    "        extra_metrics=[faithfulness_metric],  # use the faithfulness metric\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdafe80ef824bdf829744f20685ed22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ece06\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ece06_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_ece06_level0_col1\" class=\"col_heading level0 col1\" >outputs</th>\n",
       "      <th id=\"T_ece06_level0_col2\" class=\"col_heading level0 col2\" >score</th>\n",
       "      <th id=\"T_ece06_level0_col3\" class=\"col_heading level0 col3\" >justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ece06_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ece06_row0_col0\" class=\"data row0 col0\" >What is MLflow?</td>\n",
       "      <td id=\"T_ece06_row0_col1\" class=\"data row0 col1\" >MLflow is a suite of tools that simplifies the machine learning workflow by providing functionalities such as tracking and logging of parameters, metrics, and artifacts during the ML process, managing different versions of models, streamlining access to both SaaS and OSS LLM models, facilitating model analysis and comparison, providing a UI for prompt engineering, offering recommendations for structuring ML projects, and standardizing the packaging of ML code, workflows, and artifacts.</td>\n",
       "      <td id=\"T_ece06_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_ece06_row0_col3\" class=\"data row0 col3\" >The output accurately reflects all the core details mentioned in the original context. It mentions tracking and logging of parameters, metrics, and artifacts during the ML process, managing different versions of models, streamlining access to both SaaS and OSS LLM models, facilitating model analysis and comparison, providing a UI for prompt engineering, offering recommendations for structuring ML projects, and standardizing the packaging of ML code, workflows, and artifacts. All these points are directly supported by the provided context, demonstrating high faithfulness to the provided context. Therefore, the output gets a score of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ece06_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ece06_row1_col0\" class=\"data row1 col0\" >What is Delta Lake?</td>\n",
       "      <td id=\"T_ece06_row1_col1\" class=\"data row1 col1\" >Delta Lake is an open source project that allows for the creation of a Lakehouse architecture on top of data lakes. It provides features such as ACID transactions, scalable metadata handling, and unifies streaming and batch data processing.</td>\n",
       "      <td id=\"T_ece06_row1_col2\" class=\"data row1 col2\" >5</td>\n",
       "      <td id=\"T_ece06_row1_col3\" class=\"data row1 col3\" >The output accurately reflects all the core details mentioned in the original context. It correctly identifies Delta Lake as an open source project that enables the creation of a Lakehouse architecture on top of data lakes. It also correctly mentions the features of Delta Lake such as ACID transactions, scalable metadata handling, and the unification of streaming and batch data processing. There are no extraneous details or inaccuracies in the output, and it does not introduce any information that is not derived from the context. Therefore, the output demonstrates high faithfulness to the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ece06_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ece06_row2_col0\" class=\"data row2 col0\" >How to exit vim?</td>\n",
       "      <td id=\"T_ece06_row2_col1\" class=\"data row2 col1\" >After saving your changes, you can quit Vim with :q.</td>\n",
       "      <td id=\"T_ece06_row2_col2\" class=\"data row2 col2\" >5</td>\n",
       "      <td id=\"T_ece06_row2_col3\" class=\"data row2 col3\" >The output is completely faithful to the context provided. The output states \"After saving your changes, you can quit Vim with :q.\" which is exactly what the context says. There are no additional claims or contradictions in the output that are not supported by the context. Therefore, the faithfulness score is 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ece06_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ece06_row3_col0\" class=\"data row3 col0\" >How to exit emacs?</td>\n",
       "      <td id=\"T_ece06_row3_col1\" class=\"data row3 col1\" >To exit Emacs, you can use the key sequence C-x C-c (save-buffers-kill-terminal).</td>\n",
       "      <td id=\"T_ece06_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_ece06_row3_col3\" class=\"data row3 col3\" >The output is completely faithful to the context provided. The output states \"To exit Emacs, you can use the key sequence C-x C-c (save-buffers-kill-terminal).\" This is directly supported by the context, which states \"To do this, type C-x C-c (save-buffers-kill-terminal).\" Therefore, all of the claims in the output can be directly inferred from the context, warranting a score of 5.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ca47f1d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "\n",
    "# just show question outputs faithfulness/v1/score\tfaithfulness/v1/justification columns\n",
    "df[\n",
    "    [\n",
    "        \"question\",\n",
    "        \"outputs\",\n",
    "        \"faithfulness/v1/score\",\n",
    "        \"faithfulness/v1/justification\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"faithfulness/v1/score\": \"score\",\n",
    "        \"faithfulness/v1/justification\": \"justification\",\n",
    "    }\n",
    ").style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
