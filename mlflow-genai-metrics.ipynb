{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Evaluation Metrics in MLflow\n",
    "\n",
    "MLflow 2.8 introduced new [Generative AI Metrics](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#generative-ai-metrics) that use LLMs to evaluate model output text. There are a few different GenAI metrics to choose from:\n",
    "- [Answer Correctness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which compares a model's output to a ground truth answer\n",
    "- [Answer Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which evaluates how appropriate and applicable a response is with respect to the input\n",
    "- [Answer Similarity](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity), which assesses the semantic similarity of a generated response to a ground truth answer\n",
    "- [Faithfulness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness), which tests the factual similarity of a model's response to some provided context (e.g. in a RAG system)\n",
    "- [Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.relevance), which examines the output with respect to the input and provided context (e.g. in a RAG system) and rates its relevance and significance. Note that this differs from the `Answer Similarity` metric, which does not have a context component.\n",
    "\n",
    "These all work in fudamentally the same way: pick a model and (optionally) define an example, at which point you can use the new metric in the MLflow.evaluate() system. Let's go through each in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_correctness, greater_is_better=True, long_name=answer_correctness, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_correctness based on the input and output.\n",
      "A definition of answer_correctness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer correctness is evaluated on the accuracy of the provided output based on the provided targets, which is the ground truth. Scores can be assigned based on the degree of semantic similarity and factual correctness of the provided output to the provided targets, where a higher score indicates higher degree of accuracy.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Correctness: Below are the details for different scores:\n",
      "- Score 1: The output is completely incorrect. It is completely different from or contradicts the provided targets.\n",
      "- Score 2: The output demonstrates some degree of semantic similarity and includes partially correct information. However, the output still has significant discrepancies with the provided targets or inaccuracies.\n",
      "- Score 3: The output addresses a couple of aspects of the input accurately, aligning with the provided targets. However, there are still omissions or minor inaccuracies.\n",
      "- Score 4: The output is mostly correct. It provides mostly accurate information, but there may be one or more minor omissions or inaccuracies.\n",
      "- Score 5: The output is correct. It demonstrates a high degree of accuracy and semantic similarity to the targets.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Output:\n",
      "Databricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "score: 2\n",
      "justification: The output provided by the model does demonstrate some degree of semantic similarity to the targets, as it correctly identifies Databricks as a company specializing in big data and machine learning solutions. However, it fails to address the main point of the input question, which is the relationship between MLflow and Databricks. The output does not mention MLflow at all, which is a significant discrepancy with the provided targets. Therefore, the model's answer_correctness score is 2.\n",
      "        \n",
      "\n",
      "Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Output:\n",
      "MLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "score: 4\n",
      "justification: The output provided by the model is mostly correct. It correctly identifies that MLflow is a product created by Databricks. However, it does not mention that MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, which is a significant part of its function. Therefore, while the output is mostly accurate, it has a minor omission, which is why it gets a score of 4 according to the grading rubric.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_correctness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_correctness score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, answer_correctness\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow Tracking?\",\n",
    "    output=\"MLflow Tracking's API and UI log ML workflow aspects like parameters, \"\n",
    "    \"code, metrics, and artifacts. It provides a unified view of a model's \"\n",
    "    \"development, aiding team analysis. Designed for diverse environments \"\n",
    "    \"like scripts or notebooks, it simplifies result logging to files or servers, \"\n",
    "    \"enhancing run comparisons for users.\",\n",
    "    score=5,\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "        \"of parameters, code versions, metrics, and artifacts during the ML process. This \"\n",
    "        \"centralized repository captures details such as parameters, metrics, artifacts, \"\n",
    "        \"data, and environment configurations, giving teams insight into their models’ \"\n",
    "        \"evolution over time. Whether working in standalone scripts, notebooks, or other \"\n",
    "        \"environments, Tracking facilitates the logging of results either to local files or a \"\n",
    "        \"server, making it easier to compare multiple runs across different users.\"\n",
    "    },\n",
    "    justification=\"The answer gives a correct summary of MLflow tracking. \"\n",
    "    \"The answer does not include any innacuracies or significant omissions.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is the MLflow Model Registry?\",\n",
    "    output=\"MLflow's Model Registry is a version control hub for managing \"\n",
    "    \"ML model versions. It helps track model stages and facilitates a \"\n",
    "    \"smooth transition to production with a centralized model store and UI.\",\n",
    "    score=3,\n",
    "    grading_context={\n",
    "        \"targets\": \"A systematic approach to model management, the Model Registry assists \"\n",
    "        \"in handling different versions of models, discerning their current state, and \"\n",
    "        \"ensuring a smooth transition from development to production. It offers a centralized \"\n",
    "        \"model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, \"\n",
    "        \"including model lineage, versioning, stage transitions, and annotations.\"\n",
    "    },\n",
    "    justification=\"The output inaccurately characterizes the Model Registry as a version control \"\n",
    "    \"only system, thereby failing to capture its broader role in the full lifecycle management \"\n",
    "    \"of machine learning models. This overlooks key features such as collaboration, annotations, \"\n",
    "    \"and comprehensive lifecycle management, leading to a deduction in the correctness score.\"\n",
    ")\n",
    "\n",
    "example3 = EvaluationExample(\n",
    "    input=\"What is automatic logging in MLflow?\",\n",
    "    output=\"Automatic logging in MLflow is an AI-driven feature for optimizing data storage, \"\n",
    "    \"leveraging algorithms to enhance data retrieval and backups within the ML workflow.\",\n",
    "    score=1,\n",
    "    grading_context={\n",
    "        \"targets\": \"Automatic logging allows you to log metrics, parameters, and models \"\n",
    "        \"without the need for explicit log statements. There are two ways to use autologging: Call \"\n",
    "        \"mlflow.autolog() before your training code. This will enable autologging for each supported \"\n",
    "        \"library you have installed as soon as you import it. Use library-specific autolog calls for \"\n",
    "        \"each library you use in your code.\"\n",
    "    },\n",
    "    justification=\"The output erroneously represents automatic logging as a data storage optimization \"\n",
    "    \"feature, which is entirely incorrect. Automatic logging in MLflow is actually designed to log \"\n",
    "    \"metrics, parameters, and models automatically during the machine learning model training process. \"\n",
    "    \"This significant misstatement of MLflow's functionality warrants the score of 1.\"\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_correctness_metric = answer_correctness(\n",
    "    model=\"openai:/gpt-4\"#, examples=[example1, example2, example3]\n",
    ")\n",
    "\n",
    "print(answer_correctness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open source platform for managing the end-to-end machine learning lifecycle.\",\n",
    "            \"Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\",\n",
    "            \"To exit vim, press ESC to enter command mode, then type :q and press Enter.\",\n",
    "            \"To exit emacs, press Ctrl+x, then Ctrl+c.\"\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2023/11/06 11:25:17 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/06 11:25:17 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/06 11:25:46 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dba1d8e0484d15993156b540d71ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 11:25:57 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/06 11:25:57 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/06 11:25:57 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/06 11:25:57 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/06 11:25:57 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/06 11:25:57 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55adece0a6574489b89fdf75a1bbc4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.0008873457809386309,\n",
       " 'toxicity/v1/variance': 6.291812835807224e-07,\n",
       " 'toxicity/v1/p90': 0.0017894275253638628,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 12.0,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 23.380000000000003,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 16.88,\n",
       " 'ari_grade_level/v1/mean': 15.549999999999999,\n",
       " 'ari_grade_level/v1/variance': 28.0875,\n",
       " 'ari_grade_level/v1/p90': 20.66,\n",
       " 'exact_match/v1': 0.0,\n",
       " 'answer_correctness/v1/mean': 4.75,\n",
       " 'answer_correctness/v1/variance': 0.1875,\n",
       " 'answer_correctness/v1/p90': 5.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question.\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "        targets=\"ground_truth\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness_metric,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf216050a2c4e2d945783114bd65e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_86637\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_86637_level0_col0\" class=\"col_heading level0 col0\" >inputs</th>\n",
       "      <th id=\"T_86637_level0_col1\" class=\"col_heading level0 col1\" >outputs</th>\n",
       "      <th id=\"T_86637_level0_col2\" class=\"col_heading level0 col2\" >answer_correctness/v1/score</th>\n",
       "      <th id=\"T_86637_level0_col3\" class=\"col_heading level0 col3\" >answer_correctness/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_86637_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_86637_row0_col0\" class=\"data row0 col0\" >What is MLflow?</td>\n",
       "      <td id=\"T_86637_row0_col1\" class=\"data row0 col1\" >MLflow is an open-source platform used for managing the entire machine learning life cycle, including tracking experiments, packaging and reproducibility of models, and deployment of machine learning models. It provides tools and APIs to help data scientists and engineers keep track of experiments, manage and version trained models, and enable model deployment as web services in various environments.</td>\n",
       "      <td id=\"T_86637_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_86637_row0_col3\" class=\"data row0 col3\" >The output provided by the model is correct. It accurately describes MLflow as an open-source platform used for managing the entire machine learning life cycle, including tracking experiments, packaging and reproducibility of models, and deployment of machine learning models. This aligns perfectly with the provided targets, demonstrating a high degree of accuracy and semantic similarity. Therefore, according to the grading rubric, the model's answer_correctness score is 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86637_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_86637_row1_col0\" class=\"data row1 col0\" >What is Delta Lake?</td>\n",
       "      <td id=\"T_86637_row1_col1\" class=\"data row1 col1\" >Delta Lake is an open-source storage layer that sits on top of existing data lakes and provides ACID (Atomicity, Consistency, Isolation, Durability) transactions, schema enforcement, and data versioning capabilities. It enables reliable and efficient data ingestion, processing, and query operations on big data platforms.</td>\n",
       "      <td id=\"T_86637_row1_col2\" class=\"data row1 col2\" >4</td>\n",
       "      <td id=\"T_86637_row1_col3\" class=\"data row1 col3\" >The output provided by the model is mostly correct. It correctly identifies Delta Lake as an open-source storage layer that provides ACID transactions and sits on top of existing data lakes. It also correctly mentions its capabilities such as schema enforcement and data versioning. However, it does not specifically mention that Delta Lake brings ACID transactions to Apache Spark, which is a minor omission. Therefore, the model's answer_correctness score is 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86637_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_86637_row2_col0\" class=\"data row2 col0\" >How to exit vim?</td>\n",
       "      <td id=\"T_86637_row2_col1\" class=\"data row2 col1\" >To exit vim, press the \"Esc\" key to enter command mode, then type \":q\" and press \"Enter\". If you have unsaved changes, you can add an exclamation mark to force the exit, like \":q!\".</td>\n",
       "      <td id=\"T_86637_row2_col2\" class=\"data row2 col2\" >5</td>\n",
       "      <td id=\"T_86637_row2_col3\" class=\"data row2 col3\" >The output provided by the model is completely correct. It accurately describes the process of exiting vim, which aligns perfectly with the provided targets. The output mentions pressing the \"Esc\" key to enter command mode, then typing \":q\" and pressing \"Enter\". It also includes the additional information about forcing an exit with unsaved changes by adding an exclamation mark, like \":q!\". This demonstrates a high degree of accuracy and semantic similarity to the targets, which is why it gets a score of 5 according to the grading rubric.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86637_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_86637_row3_col0\" class=\"data row3 col0\" >How to exit emacs?</td>\n",
       "      <td id=\"T_86637_row3_col1\" class=\"data row3 col1\" >To exit Emacs, you can use the keyboard command \"Ctrl-x Ctrl-c\" or type \"M-x save-buffers-kill-emacs\" and press Enter.</td>\n",
       "      <td id=\"T_86637_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_86637_row3_col3\" class=\"data row3 col3\" >The output provided by the model is correct. It accurately provides the keyboard command \"Ctrl-x Ctrl-c\" to exit Emacs, which aligns perfectly with the provided targets. Additionally, the model provides an extra command \"M-x save-buffers-kill-emacs\" to exit Emacs, which is also correct, although it was not mentioned in the targets. Therefore, the model's answer_correctness score is 5 according to the grading rubric.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2dda94a10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "df[[\"inputs\", \"outputs\", \"answer_correctness/v1/score\", \"answer_correctness/v1/justification\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import answer_relevance\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow Tracking?\",\n",
    "    output=\"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "    \"of parameters, code versions, metrics, and artifacts during the ML process. \"\n",
    "    \"This centralized repository captures details such as parameters, metrics, \"\n",
    "    \"artifacts, data, and environment configurations, giving teams insight into their \"\n",
    "    \"models’ evolution over time.\",\n",
    "    score=5,\n",
    "    justification=\"The answer directly addresses the input question and  \"\n",
    "    \"provides a concise and clear description of MLflow Tracking.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is MLflow Model Registry?\",\n",
    "    output=\"MLflow Model Registry is a component of MLflow that helps in managing \"\n",
    "    \"and deploying models in production. It provides versioning and stage transitions. \"\n",
    "    \"MLflow also has a model evaluation feature for evaluating ML models.\",\n",
    "    score=3,\n",
    "    justification=\"The answer provides a general idea about MLflow Model Registry and \"\n",
    "    \"includes correct details about versioning and stage transitions. The mention of model evaluation \"\n",
    "    \"is irrelevant to the input question, hence the score of 3.\",\n",
    ")\n",
    "\n",
    "example3 = EvaluationExample(\n",
    "    input=\"What is automatic logging in MLflow?\",\n",
    "    output=\"Delta Lake is an open-source storage layer that brings ACID transactions to Apache \"\n",
    "    \"Spark and big data workloads.\",\n",
    "    score=1,\n",
    "    justification=\"The output is completely irrelevant to the input question about \"\n",
    "    \"automatic logging in MLflow, hence the score of 1.\",\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_relevance_metric = answer_relevance(model=\"openai:/gpt-4\", examples=[example1, example2, example3])\n",
    "\n",
    "print(answer_relevance_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question.\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "        targets=\"ground_truth\",\n",
    "        extra_metrics=[\n",
    "         #   answer_relevance_metric,\n",
    "            answer_correctness_metric,\n",
    "        ],  # use the answer similarity metric created above\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "\n",
    "# just show question outputs faithfulness/v1/score\tfaithfulness/v1/justification columns\n",
    "#df[[\"inputs\", \"outputs\", \"answer_relevance/v1/score\", \"answer_relevance/v1/justification\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "The [*faithfulness*](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness) metric assesses \"how factually consistent the output is to the context.\" So for this metric, we require input, output, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"What is MLflow Tracking?\",\n",
    "        output=\"MLflow Tracking offers an API and UI for centralized logging of machine \"\n",
    "        \"learning parameters, code versions, metrics, and artifacts, supporting a variety \"\n",
    "        \"of environments and enabling easier comparison of multiple runs across users.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a clear answer including only details from the provided context\",\n",
    "        grading_context={\n",
    "            \"context\": \"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "            \"of parameters, code versions, metrics, and artifacts during the ML process. This \"\n",
    "            \"centralized repository captures details such as parameters, metrics, artifacts, \"\n",
    "            \"data, and environment configurations, giving teams insight into their models’ \"\n",
    "            \"evolution over time. Whether working in standalone scripts, notebooks, or other \"\n",
    "            \"environments, Tracking facilitates the logging of results either to local files or a \"\n",
    "            \"server, making it easier to compare multiple runs across different users.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is MLflow Model Registry?\",\n",
    "        output=\"The Model Registry centralizes MLflow Models in a repository, offering APIs, a \"\n",
    "        \"user interface, and features like versioning, state tracking, and annotations, as \"\n",
    "        \"well as the ability to archive, delete, and search models for a seamless transition \"\n",
    "        \"from development to production and ongoing management.\",\n",
    "        score=3,\n",
    "        justification=\"The output receives a score of 3 rather than a lower score like 1 \"\n",
    "        \"because it does accurately reflect most of the core details mentioned in the \"\n",
    "        \"original context such as centralized storage, APIs, UI, versioning, state tracking, \"\n",
    "        \"and annotations. However, it introduces extraneous details like the ability to \"\n",
    "        \"'archive, delete, and search models,' which are not derived from the context. \"\n",
    "        \"Additionally, it substitutes 'repository' for 'store' and introduces 'ongoing \"\n",
    "        \"management,' slight deviations from the original description. These inaccuracies \"\n",
    "        \"and additions are not fundamentally wrong, but they are not faithful to the original \"\n",
    "        \"context, warranting a deduction in score.\",\n",
    "        grading_context={\n",
    "            \"context\": \"A systematic approach to model management, the Model Registry assists \"\n",
    "            \"in handling different versions of models, discerning their current state, and \"\n",
    "            \"ensuring a smooth transition from development to production. It offers a centralized \"\n",
    "            \"model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, \"\n",
    "            \"including model lineage, versioning, stage transitions, and annotations.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is automatic logging in MLflow?\",\n",
    "        output=\"Automatic logging in MLflow is a machine learning technique used to automate the \"\n",
    "        \"process of cutting down trees for lumber.\",\n",
    "        score=1,\n",
    "        justification=\"The output is entirely incorrect and fails to utilize the given context. \"\n",
    "        \"The context is about MLflow's automatic logging for metrics, parameters, and models in \"\n",
    "        \"machine learning projects. Instead, the output discusses using machine learning to automate \"\n",
    "        \"the process of cutting down trees, which is completely unrelated to the actual context provided.\",\n",
    "        grading_context={\n",
    "            \"context\": \"Automatic logging allows you to log metrics, parameters, and models without the \"\n",
    "            \"need for explicit log statements. There are two ways to use autologging: Call mlflow.autolog() \"\n",
    "            \"before your training code. This will enable autologging for each supported library you have \"\n",
    "            \"installed as soon as you import it. Use library-specific autolog calls for each library you use \"\n",
    "            \"in your code.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=\"openai:/gpt-4\", examples=faithfulness_examples\n",
    ")\n",
    "print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"context\": [\n",
    "            \"MLflow, at its core, provides a suite of tools aimed at simplifying the ML \"\n",
    "            \"workflow. It is tailored to assist ML practitioners throughout the various \"\n",
    "            \"stages of ML development and deployment. Despite its expansive offerings, \"\n",
    "            \"MLflow’s functionalities are rooted in several foundational components:\"\n",
    "            \"Tracking: MLflow Tracking provides both an API and UI dedicated to the \"\n",
    "            \"logging of parameters, code versions, metrics, and artifacts during the ML \"\n",
    "            \"process. This centralized repository captures details such as parameters, \"\n",
    "            \"metrics, artifacts, data, and environment configurations, giving teams \"\n",
    "            \"insight into their models’ evolution over time. Whether working in standalone \"\n",
    "            \"scripts, notebooks, or other environments, Tracking facilitates the logging \"\n",
    "            \"of results either to local files or a server, making it easier to compare \"\n",
    "            \"multiple runs across different users.\"\n",
    "            \"Model Registry: A systematic approach to model management, the Model Registry \"\n",
    "            \"assists in handling different versions of models, discerning their current \"\n",
    "            \"state, and ensuring a smooth transition from development to production. It \"\n",
    "            \"offers a centralized model store, APIs, and UI to collaboratively manage an \"\n",
    "            \"MLflow Model’s full lifecycle, including model lineage, versioning, stage \"\n",
    "            \"transitions, and annotations.\"\n",
    "            \"AI Gateway: This server, equipped with a set of standardized APIs, streamlines \"\n",
    "            \"access to both SaaS and OSS LLM models. It serves as a unified interface, \"\n",
    "            \"bolstering security through authenticated access, and offers a common set of \"\n",
    "            \"APIs for prominent LLMs.\"\n",
    "            \"Evaluate: Designed for in-depth model analysis, this set of tools facilitates \"\n",
    "            \"objective model comparison, be it traditional ML algorithms or cutting-edge \"\n",
    "            \"LLMs.\"\n",
    "            \"Prompt Engineering UI: A dedicated environment for prompt engineering, this \"\n",
    "            \"UI-centric component provides a space for prompt experimentation, refinement, \"\n",
    "            \"evaluation, testing, and deployment.\"\n",
    "            \"Recipes: Serving as a guide for structuring ML projects, Recipes, while \"\n",
    "            \"offering recommendations, are focused on ensuring functional end results \"\n",
    "            \"optimized for real-world deployment scenarios.\"\n",
    "            \"Projects: MLflow Projects standardize the packaging of ML code, workflows, \"\n",
    "            \"and artifacts, akin to an executable. Each project, be it a directory with \"\n",
    "            \"code or a Git repository, employs a descriptor or convention to define its \"\n",
    "            \"dependencies and execution method.\",\n",
    "            \"Delta Lake is an open source project that enables building a Lakehouse \"\n",
    "            \"architecture on top of data lakes. Delta Lake provides ACID transactions, \"\n",
    "            \"scalable metadata handling, and unifies streaming and batch data processing \"\n",
    "            \"on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\"\n",
    "            \"Specifically, Delta Lake offers:\"\n",
    "            \"ACID transactions on Spark: Serializable isolation levels ensure that readers \"\n",
    "            \"never see inconsistent data.\"\n",
    "            \"Scalable metadata handling: Leverages Spark distributed processing power to \"\n",
    "            \"handle all the metadata for petabyte-scale tables with billions of files at ease.\"\n",
    "            \"Streaming and batch unification: A table in Delta Lake is a batch table as well \"\n",
    "            \"as a streaming source and sink. Streaming data ingest, batch historic backfill, \"\n",
    "            \"interactive queries all just work out of the box.\"\n",
    "            \"Schema enforcement: Automatically handles schema variations to prevent insertion \"\n",
    "            \"of bad records during ingestion.\"\n",
    "            \"Time travel: Data versioning enables rollbacks, full historical audit trails, \"\n",
    "            \"and reproducible machine learning experiments.\"\n",
    "            \"Upserts and deletes: Supports merge, update and delete operations to enable \"\n",
    "            \"complex use cases like change-data-capture, slowly-changing-dimension (SCD) \"\n",
    "            \"operations, streaming upserts, and so on.\",\n",
    "            \"After saving your changes, you can quit Vim with :q. Or the saving and \"\n",
    "            \"quitting can be combined into one operation with :wq or :x.\"\n",
    "            \"If you want to discard any changes, enter :q! to quit Vim without saving.\",\n",
    "            \"C-x C-c\\n\\n    Kill Emacs (save-buffers-kill-terminal).\"\n",
    "            \"C-z\\n\\n    On a text terminal, suspend Emacs; on a graphical display, \"\n",
    "            \"iconify (or “minimize”) the selected frame (suspend-frame).\"\n",
    "            \"Killing Emacs means terminating the Emacs program. To do this, type \"\n",
    "            \"C-x C-c (save-buffers-kill-terminal). A two-character key sequence is \"\n",
    "            \"used to make it harder to type by accident. If there are any modified \"\n",
    "            \"file-visiting buffers when you type C-x C-c, Emacs first offers to save \"\n",
    "            \"these buffers. If you do not save them all, it asks for confirmation \"\n",
    "            \"again, since the unsaved changes will be lost. Emacs also asks for \"\n",
    "            \"confirmation if any subprocesses are still running, since killing \"\n",
    "            \"Emacs will also kill the subprocesses (see Running Shell Commands from Emacs).\"\n",
    "            \"C-x C-c behaves specially if you are using Emacs as a server. If you \"\n",
    "            \"type it from a client frame, it closes the client connection. See Using \"\n",
    "            \"Emacs as a Server.\",\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question using only the information provided in the context.\"\n",
    "    basic_context_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_context_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        predictions=\"result\",\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"question\",\n",
    "                \"context\": \"context\"\n",
    "            }\n",
    "        },\n",
    "        extra_metrics=[faithfulness_metric],  # use the faithfulness metric\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "\n",
    "# just show question outputs faithfulness/v1/score\tfaithfulness/v1/justification columns\n",
    "df[[\"question\", \"outputs\", \"faithfulness/v1/score\", \"faithfulness/v1/justification\"]].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
