{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Evaluation Metrics in MLflow\n",
    "\n",
    "MLflow 2.8 introduced new [Generative AI Metrics](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#generative-ai-metrics) that use LLMs to evaluate model output text. There are a few different GenAI metrics to choose from:\n",
    "- [Answer Correctness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which compares a model's output to a ground truth answer\n",
    "- [Answer Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which evaluates how appropriate and applicable a response is with respect to the input\n",
    "- [Answer Similarity](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity), which assesses the semantic similarity of a generated response to a ground truth answer\n",
    "- [Faithfulness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness), which tests the factual similarity of a model's response to some provided context (e.g. in a RAG system)\n",
    "- [Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.relevance), which examines the output with respect to the input and provided context (e.g. in a RAG system) and rates its relevance and significance. Note that this differs from the `Answer Similarity` metric, which does not have a context component.\n",
    "\n",
    "These all work in fudamentally the same way: pick a model and (optionally) define an example, at which point you can use the new metric in the MLflow.evaluate() system. Let's go through each in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_relevance, greater_is_better=True, long_name=answer_relevance, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_relevance based on the input and output.\n",
      "A definition of answer_relevance and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer relevance measures the appropriateness and applicability of the output with respect to the input. Scores should reflect the extent to which the output directly addresses the question provided in the input, and give lower scores for incomplete or redundant output.\n",
      "\n",
      "Grading rubric:\n",
      "Answer relevance: Please give a score from 1-5 based on the degree of relevance to the input, where the lowest and highest scores are defined as follows:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the input.\n",
      "- Score 5: The output addresses all aspects of the question and all parts of the output are meaningful and relevant to the question.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "What is MLflow Tracking?\n",
      "\n",
      "Output:\n",
      "MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time.\n",
      "\n",
      "\n",
      "\n",
      "score: 5\n",
      "justification: The answer directly addresses the input question and  provides a concise and clear description of MLflow Tracking.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is MLflow Model Registry?\n",
      "\n",
      "Output:\n",
      "MLflow Model Registry is a component of MLflow that helps in managing and deploying models in production. It provides versioning and stage transitions. MLflow also has a model evaluation feature for evaluating ML models.\n",
      "\n",
      "\n",
      "\n",
      "score: 3\n",
      "justification: The answer provides a general idea about MLflow Model Registry and includes correct details about versioning and stage transitions. The mention of model evaluation is irrelevant to the input question, hence the score of 3.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is automatic logging in MLflow?\n",
      "\n",
      "Output:\n",
      "Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\n",
      "\n",
      "\n",
      "\n",
      "score: 1\n",
      "justification: The output is completely irrelevant to the input question about automatic logging in MLflow, hence the score of 1.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_relevance based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_relevance score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, answer_relevance\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow Tracking?\",\n",
    "    output=\"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "    \"of parameters, code versions, metrics, and artifacts during the ML process. \"\n",
    "    \"This centralized repository captures details such as parameters, metrics, \"\n",
    "    \"artifacts, data, and environment configurations, giving teams insight into their \"\n",
    "    \"models’ evolution over time.\",\n",
    "    score=5,\n",
    "    justification=\"The answer directly addresses the input question and  \"\n",
    "    \"provides a concise and clear description of MLflow Tracking.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is MLflow Model Registry?\",\n",
    "    output=\"MLflow Model Registry is a component of MLflow that helps in managing \"\n",
    "    \"and deploying models in production. It provides versioning and stage transitions. \"\n",
    "    \"MLflow also has a model evaluation feature for evaluating ML models.\",\n",
    "    score=3,\n",
    "    justification=\"The answer provides a general idea about MLflow Model Registry and \"\n",
    "    \"includes correct details about versioning and stage transitions. The mention of model evaluation \"\n",
    "    \"is irrelevant to the input question, hence the score of 3.\",\n",
    ")\n",
    "\n",
    "example3 = EvaluationExample(\n",
    "    input=\"What is automatic logging in MLflow?\",\n",
    "    output=\"Delta Lake is an open-source storage layer that brings ACID transactions to Apache \"\n",
    "    \"Spark and big data workloads.\",\n",
    "    score=1,\n",
    "    justification=\"The output is completely irrelevant to the input question about \"\n",
    "    \"automatic logging in MLflow, hence the score of 1.\",\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_relevance_metric = answer_relevance(model=\"openai:/gpt-4\", examples=[example1, example2, example3])\n",
    "\n",
    "print(answer_relevance_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open source platform for managing the end-to-end machine learning lifecycle.\",\n",
    "            \"Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\",\n",
    "            \"To exit vim, press ESC to enter command mode, then type :q and press Enter.\",\n",
    "            \"To exit emacs, press Ctrl+x, then Ctrl+c.\"\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/01 16:09:30 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/01 16:09:30 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/01 16:09:52 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca54c7b19b8455b9e8b48d15bcc5d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/01 16:10:03 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/01 16:10:03 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/01 16:10:03 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/01 16:10:03 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/01 16:10:03 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/01 16:10:03 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_relevance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062574f0a6a947a2af7b33c6237d2748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.00127542274640291,\n",
       " 'toxicity/v1/variance': 2.45783261476677e-06,\n",
       " 'toxicity/v1/p90': 0.0030177891778294004,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 10.4,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 23.944999999999993,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 15.76,\n",
       " 'ari_grade_level/v1/mean': 13.524999999999999,\n",
       " 'ari_grade_level/v1/variance': 51.786875,\n",
       " 'ari_grade_level/v1/p90': 20.84,\n",
       " 'answer_relevance/v1/mean': 5.0,\n",
       " 'answer_relevance/v1/variance': 0.0,\n",
       " 'answer_relevance/v1/p90': 5.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question.\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[answer_relevance_metric],  # use the answer similarity metric created above\n",
    "\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c3134f891e4c549463bb08add8f3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_20f33\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_20f33_level0_col0\" class=\"col_heading level0 col0\" >inputs</th>\n",
       "      <th id=\"T_20f33_level0_col1\" class=\"col_heading level0 col1\" >outputs</th>\n",
       "      <th id=\"T_20f33_level0_col2\" class=\"col_heading level0 col2\" >answer_relevance/v1/score</th>\n",
       "      <th id=\"T_20f33_level0_col3\" class=\"col_heading level0 col3\" >answer_relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_20f33_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_20f33_row0_col0\" class=\"data row0 col0\" >What is MLflow?</td>\n",
       "      <td id=\"T_20f33_row0_col1\" class=\"data row0 col1\" >MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools and frameworks to track experiments, manage models, and deploy them into production environments, making it easier to reproduce and share machine learning projects.</td>\n",
       "      <td id=\"T_20f33_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_20f33_row0_col3\" class=\"data row0 col3\" >The output provides a comprehensive and accurate explanation of what MLflow is, directly addressing the input question. It covers the main aspects of MLflow, including its role in managing the machine learning lifecycle, tracking experiments, managing models, and deploying them into production environments. All parts of the output are meaningful and relevant to the question, hence the score of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20f33_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_20f33_row1_col0\" class=\"data row1 col0\" >What is Delta Lake?</td>\n",
       "      <td id=\"T_20f33_row1_col1\" class=\"data row1 col1\" >Delta Lake is an open-source storage layer that provides ACID transactions, scalable meta-data management, and schema evolution for big data processing and analytics on top of cloud storage systems like Apache Hadoop or Amazon S3.</td>\n",
       "      <td id=\"T_20f33_row1_col2\" class=\"data row1 col2\" >5</td>\n",
       "      <td id=\"T_20f33_row1_col3\" class=\"data row1 col3\" >The output directly addresses the input question about Delta Lake. It provides a comprehensive and accurate description of Delta Lake, including its key features and the systems it can be used with. All parts of the output are meaningful and relevant to the question, hence the score of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20f33_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_20f33_row2_col0\" class=\"data row2 col0\" >How to exit vim?</td>\n",
       "      <td id=\"T_20f33_row2_col1\" class=\"data row2 col1\" >To exit vim, you can use the command `:q` and press Enter.</td>\n",
       "      <td id=\"T_20f33_row2_col2\" class=\"data row2 col2\" >5</td>\n",
       "      <td id=\"T_20f33_row2_col3\" class=\"data row2 col3\" >The output directly answers the input question about how to exit vim. It provides the exact command needed to perform the action, which is both relevant and meaningful to the question. Therefore, the answer relevance score is 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20f33_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_20f33_row3_col0\" class=\"data row3 col0\" >How to exit emacs?</td>\n",
       "      <td id=\"T_20f33_row3_col1\" class=\"data row3 col1\" >To exit Emacs, you can use the command `C-x C-c` or `M-x save-buffers-kill-terminal`.</td>\n",
       "      <td id=\"T_20f33_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_20f33_row3_col3\" class=\"data row3 col3\" >The output directly addresses the input question by providing the commands to exit Emacs. All parts of the output are meaningful and relevant to the question, hence the score of 5.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2e0e376d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "\n",
    "# just show question outputs faithfulness/v1/score\tfaithfulness/v1/justification columns\n",
    "df[[\"inputs\", \"outputs\", \"answer_relevance/v1/score\", \"answer_relevance/v1/justification\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "The [*faithfulness*](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness) metric assesses \"how factually consistent the output is to the context.\" So for this metric, we require input, output, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called faithfulness based on the input and output.\n",
      "A definition of faithfulness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n",
      "\n",
      "Grading rubric:\n",
      "Faithfulness: Below are the details for different scores:\n",
      "- Score 1: None of the claims in the output can be inferred from the provided context.\n",
      "- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n",
      "- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n",
      "- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n",
      "- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "What is MLflow Tracking?\n",
      "\n",
      "Output:\n",
      "MLflow Tracking offers an API and UI for centralized logging of machine learning parameters, code versions, metrics, and artifacts, supporting a variety of environments and enabling easier comparison of multiple runs across users.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time. Whether working in standalone scripts, notebooks, or other environments, Tracking facilitates the logging of results either to local files or a server, making it easier to compare multiple runs across different users.\n",
      "\n",
      "score: 5\n",
      "justification: The output provides a clear answer including only details from the provided context\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is MLflow Model Registry?\n",
      "\n",
      "Output:\n",
      "The Model Registry centralizes MLflow Models in a repository, offering APIs, a user interface, and features like versioning, state tracking, and annotations, as well as the ability to archive, delete, and search models for a seamless transition from development to production and ongoing management.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "A systematic approach to model management, the Model Registry assists in handling different versions of models, discerning their current state, and ensuring a smooth transition from development to production. It offers a centralized model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, including model lineage, versioning, stage transitions, and annotations.\n",
      "\n",
      "score: 3\n",
      "justification: The output receives a score of 3 rather than a lower score like 1 because it does accurately reflect most of the core details mentioned in the original context such as centralized storage, APIs, UI, versioning, state tracking, and annotations. However, it introduces extraneous details like the ability to 'archive, delete, and search models,' which are not derived from the context. Additionally, it substitutes 'repository' for 'store' and introduces 'ongoing management,' slight deviations from the original description. These inaccuracies and additions are not fundamentally wrong, but they are not faithful to the original context, warranting a deduction in score.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is automatic logging in MLflow?\n",
      "\n",
      "Output:\n",
      "Automatic logging in MLflow is a machine learning technique used to automate the process of cutting down trees for lumber.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "Automatic logging allows you to log metrics, parameters, and models without the need for explicit log statements. There are two ways to use autologging: Call mlflow.autolog() before your training code. This will enable autologging for each supported library you have installed as soon as you import it. Use library-specific autolog calls for each library you use in your code.\n",
      "\n",
      "score: 1\n",
      "justification: The output is entirely incorrect and fails to utilize the given context. The context is about MLflow's automatic logging for metrics, parameters, and models in machine learning projects. Instead, the output discusses using machine learning to automate the process of cutting down trees, which is completely unrelated to the actual context provided.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's faithfulness score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"What is MLflow Tracking?\",\n",
    "        output=\"MLflow Tracking offers an API and UI for centralized logging of machine \"\n",
    "        \"learning parameters, code versions, metrics, and artifacts, supporting a variety \"\n",
    "        \"of environments and enabling easier comparison of multiple runs across users.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a clear answer including only details from the provided context\",\n",
    "        grading_context={\n",
    "            \"context\": \"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "            \"of parameters, code versions, metrics, and artifacts during the ML process. This \"\n",
    "            \"centralized repository captures details such as parameters, metrics, artifacts, \"\n",
    "            \"data, and environment configurations, giving teams insight into their models’ \"\n",
    "            \"evolution over time. Whether working in standalone scripts, notebooks, or other \"\n",
    "            \"environments, Tracking facilitates the logging of results either to local files or a \"\n",
    "            \"server, making it easier to compare multiple runs across different users.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is MLflow Model Registry?\",\n",
    "        output=\"The Model Registry centralizes MLflow Models in a repository, offering APIs, a \"\n",
    "        \"user interface, and features like versioning, state tracking, and annotations, as \"\n",
    "        \"well as the ability to archive, delete, and search models for a seamless transition \"\n",
    "        \"from development to production and ongoing management.\",\n",
    "        score=3,\n",
    "        justification=\"The output receives a score of 3 rather than a lower score like 1 \"\n",
    "        \"because it does accurately reflect most of the core details mentioned in the \"\n",
    "        \"original context such as centralized storage, APIs, UI, versioning, state tracking, \"\n",
    "        \"and annotations. However, it introduces extraneous details like the ability to \"\n",
    "        \"'archive, delete, and search models,' which are not derived from the context. \"\n",
    "        \"Additionally, it substitutes 'repository' for 'store' and introduces 'ongoing \"\n",
    "        \"management,' slight deviations from the original description. These inaccuracies \"\n",
    "        \"and additions are not fundamentally wrong, but they are not faithful to the original \"\n",
    "        \"context, warranting a deduction in score.\",\n",
    "        grading_context={\n",
    "            \"context\": \"A systematic approach to model management, the Model Registry assists \"\n",
    "            \"in handling different versions of models, discerning their current state, and \"\n",
    "            \"ensuring a smooth transition from development to production. It offers a centralized \"\n",
    "            \"model store, APIs, and UI to collaboratively manage an MLflow Model’s full lifecycle, \"\n",
    "            \"including model lineage, versioning, stage transitions, and annotations.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is automatic logging in MLflow?\",\n",
    "        output=\"Automatic logging in MLflow is a machine learning technique used to automate the \"\n",
    "        \"process of cutting down trees for lumber.\",\n",
    "        score=1,\n",
    "        justification=\"The output is entirely incorrect and fails to utilize the given context. \"\n",
    "        \"The context is about MLflow's automatic logging for metrics, parameters, and models in \"\n",
    "        \"machine learning projects. Instead, the output discusses using machine learning to automate \"\n",
    "        \"the process of cutting down trees, which is completely unrelated to the actual context provided.\",\n",
    "        grading_context={\n",
    "            \"context\": \"Automatic logging allows you to log metrics, parameters, and models without the \"\n",
    "            \"need for explicit log statements. There are two ways to use autologging: Call mlflow.autolog() \"\n",
    "            \"before your training code. This will enable autologging for each supported library you have \"\n",
    "            \"installed as soon as you import it. Use library-specific autolog calls for each library you use \"\n",
    "            \"in your code.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=\"openai:/gpt-4\", examples=faithfulness_examples\n",
    ")\n",
    "print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"context\": [\n",
    "            \"MLflow, at its core, provides a suite of tools aimed at simplifying the ML \"\n",
    "            \"workflow. It is tailored to assist ML practitioners throughout the various \"\n",
    "            \"stages of ML development and deployment. Despite its expansive offerings, \"\n",
    "            \"MLflow’s functionalities are rooted in several foundational components:\"\n",
    "            \"Tracking: MLflow Tracking provides both an API and UI dedicated to the \"\n",
    "            \"logging of parameters, code versions, metrics, and artifacts during the ML \"\n",
    "            \"process. This centralized repository captures details such as parameters, \"\n",
    "            \"metrics, artifacts, data, and environment configurations, giving teams \"\n",
    "            \"insight into their models’ evolution over time. Whether working in standalone \"\n",
    "            \"scripts, notebooks, or other environments, Tracking facilitates the logging \"\n",
    "            \"of results either to local files or a server, making it easier to compare \"\n",
    "            \"multiple runs across different users.\"\n",
    "            \"Model Registry: A systematic approach to model management, the Model Registry \"\n",
    "            \"assists in handling different versions of models, discerning their current \"\n",
    "            \"state, and ensuring a smooth transition from development to production. It \"\n",
    "            \"offers a centralized model store, APIs, and UI to collaboratively manage an \"\n",
    "            \"MLflow Model’s full lifecycle, including model lineage, versioning, stage \"\n",
    "            \"transitions, and annotations.\"\n",
    "            \"AI Gateway: This server, equipped with a set of standardized APIs, streamlines \"\n",
    "            \"access to both SaaS and OSS LLM models. It serves as a unified interface, \"\n",
    "            \"bolstering security through authenticated access, and offers a common set of \"\n",
    "            \"APIs for prominent LLMs.\"\n",
    "            \"Evaluate: Designed for in-depth model analysis, this set of tools facilitates \"\n",
    "            \"objective model comparison, be it traditional ML algorithms or cutting-edge \"\n",
    "            \"LLMs.\"\n",
    "            \"Prompt Engineering UI: A dedicated environment for prompt engineering, this \"\n",
    "            \"UI-centric component provides a space for prompt experimentation, refinement, \"\n",
    "            \"evaluation, testing, and deployment.\"\n",
    "            \"Recipes: Serving as a guide for structuring ML projects, Recipes, while \"\n",
    "            \"offering recommendations, are focused on ensuring functional end results \"\n",
    "            \"optimized for real-world deployment scenarios.\"\n",
    "            \"Projects: MLflow Projects standardize the packaging of ML code, workflows, \"\n",
    "            \"and artifacts, akin to an executable. Each project, be it a directory with \"\n",
    "            \"code or a Git repository, employs a descriptor or convention to define its \"\n",
    "            \"dependencies and execution method.\",\n",
    "            \"Delta Lake is an open source project that enables building a Lakehouse \"\n",
    "            \"architecture on top of data lakes. Delta Lake provides ACID transactions, \"\n",
    "            \"scalable metadata handling, and unifies streaming and batch data processing \"\n",
    "            \"on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\"\n",
    "            \"Specifically, Delta Lake offers:\"\n",
    "            \"ACID transactions on Spark: Serializable isolation levels ensure that readers \"\n",
    "            \"never see inconsistent data.\"\n",
    "            \"Scalable metadata handling: Leverages Spark distributed processing power to \"\n",
    "            \"handle all the metadata for petabyte-scale tables with billions of files at ease.\"\n",
    "            \"Streaming and batch unification: A table in Delta Lake is a batch table as well \"\n",
    "            \"as a streaming source and sink. Streaming data ingest, batch historic backfill, \"\n",
    "            \"interactive queries all just work out of the box.\"\n",
    "            \"Schema enforcement: Automatically handles schema variations to prevent insertion \"\n",
    "            \"of bad records during ingestion.\"\n",
    "            \"Time travel: Data versioning enables rollbacks, full historical audit trails, \"\n",
    "            \"and reproducible machine learning experiments.\"\n",
    "            \"Upserts and deletes: Supports merge, update and delete operations to enable \"\n",
    "            \"complex use cases like change-data-capture, slowly-changing-dimension (SCD) \"\n",
    "            \"operations, streaming upserts, and so on.\",\n",
    "            \"After saving your changes, you can quit Vim with :q. Or the saving and \"\n",
    "            \"quitting can be combined into one operation with :wq or :x.\"\n",
    "            \"If you want to discard any changes, enter :q! to quit Vim without saving.\",\n",
    "            \"C-x C-c\\n\\n    Kill Emacs (save-buffers-kill-terminal).\"\n",
    "            \"C-z\\n\\n    On a text terminal, suspend Emacs; on a graphical display, \"\n",
    "            \"iconify (or “minimize”) the selected frame (suspend-frame).\"\n",
    "            \"Killing Emacs means terminating the Emacs program. To do this, type \"\n",
    "            \"C-x C-c (save-buffers-kill-terminal). A two-character key sequence is \"\n",
    "            \"used to make it harder to type by accident. If there are any modified \"\n",
    "            \"file-visiting buffers when you type C-x C-c, Emacs first offers to save \"\n",
    "            \"these buffers. If you do not save them all, it asks for confirmation \"\n",
    "            \"again, since the unsaved changes will be lost. Emacs also asks for \"\n",
    "            \"confirmation if any subprocesses are still running, since killing \"\n",
    "            \"Emacs will also kill the subprocesses (see Running Shell Commands from Emacs).\"\n",
    "            \"C-x C-c behaves specially if you are using Emacs as a server. If you \"\n",
    "            \"type it from a client frame, it closes the client connection. See Using \"\n",
    "            \"Emacs as a Server.\",\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2023/11/01 16:10:13 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/11/01 16:10:13 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/11/01 16:10:18 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5750201d68a24573b929dff7bb2bc4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3787: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/daniel.liden/miniconda3/envs/mlops/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "2023/11/01 16:10:35 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/11/01 16:10:35 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/11/01 16:10:35 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/11/01 16:10:35 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/11/01 16:10:35 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/11/01 16:10:35 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc9e80310814ce5918b9b4addb8e97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'toxicity/v1/mean': 0.00034903426058008336,\n",
       " 'toxicity/v1/variance': 5.5781491372641646e-08,\n",
       " 'toxicity/v1/p90': 0.0006194831104949117,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'flesch_kincaid_grade_level/v1/mean': 7.875,\n",
       " 'flesch_kincaid_grade_level/v1/variance': 21.431874999999998,\n",
       " 'flesch_kincaid_grade_level/v1/p90': 13.01,\n",
       " 'ari_grade_level/v1/mean': 8.675,\n",
       " 'ari_grade_level/v1/variance': 47.891875000000006,\n",
       " 'ari_grade_level/v1/p90': 16.05,\n",
       " 'faithfulness/v1/mean': 5.0,\n",
       " 'faithfulness/v1/variance': 0.0,\n",
       " 'faithfulness/v1/p90': 5.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question using only the information provided in the context.\"\n",
    "    basic_context_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_context_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        predictions=\"result\",\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"question\",\n",
    "                \"context\": \"context\"\n",
    "            }\n",
    "        },\n",
    "        extra_metrics=[faithfulness_metric],  # use the faithfulness metric\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11382bee1f5940338db66b90e04e9a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_93f62\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_93f62_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_93f62_level0_col1\" class=\"col_heading level0 col1\" >outputs</th>\n",
       "      <th id=\"T_93f62_level0_col2\" class=\"col_heading level0 col2\" >faithfulness/v1/score</th>\n",
       "      <th id=\"T_93f62_level0_col3\" class=\"col_heading level0 col3\" >faithfulness/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_93f62_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_93f62_row0_col0\" class=\"data row0 col0\" >What is MLflow?</td>\n",
       "      <td id=\"T_93f62_row0_col1\" class=\"data row0 col1\" >MLflow is a suite of tools aimed at simplifying the machine learning workflow. It provides functionalities such as ML model tracking, model management, accessing machine learning models, model evaluation, prompt engineering UI, ML project structuring, and packaging.</td>\n",
       "      <td id=\"T_93f62_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_93f62_row0_col3\" class=\"data row0 col3\" >The output accurately reflects the context provided. All the claims made in the output such as MLflow being a suite of tools aimed at simplifying the machine learning workflow, providing functionalities like ML model tracking, model management, accessing machine learning models, model evaluation, prompt engineering UI, ML project structuring, and packaging can be directly inferred from the context. Therefore, the output demonstrates high faithfulness to the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93f62_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_93f62_row1_col0\" class=\"data row1 col0\" >What is Delta Lake?</td>\n",
       "      <td id=\"T_93f62_row1_col1\" class=\"data row1 col1\" >Delta Lake is an open source project that enables building a Lakehouse architecture on top of data lakes. It provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes.</td>\n",
       "      <td id=\"T_93f62_row1_col2\" class=\"data row1 col2\" >5</td>\n",
       "      <td id=\"T_93f62_row1_col3\" class=\"data row1 col3\" >The output is entirely consistent with the provided context. It accurately describes the role of a data scientist as someone who analyzes and interprets complex digital data, such as the usage statistics of a website, to assist a business in its decision-making process. All the claims in the output can be directly inferred from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93f62_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_93f62_row2_col0\" class=\"data row2 col0\" >How to exit vim?</td>\n",
       "      <td id=\"T_93f62_row2_col1\" class=\"data row2 col1\" >You can exit Vim by entering :q, :wq, or :x.</td>\n",
       "      <td id=\"T_93f62_row2_col2\" class=\"data row2 col2\" >5</td>\n",
       "      <td id=\"T_93f62_row2_col3\" class=\"data row2 col3\" >The output accurately reflects the information provided in the context. All the commands mentioned in the output for exiting Vim, i.e., :q, :wq, and :x, are directly supported by the context. There is no information in the output that contradicts or is inconsistent with the context. Therefore, the output demonstrates high faithfulness to the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93f62_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_93f62_row3_col0\" class=\"data row3 col0\" >How to exit emacs?</td>\n",
       "      <td id=\"T_93f62_row3_col1\" class=\"data row3 col1\" >To exit emacs, you can type C-x C-c on your keyboard. This will terminate the Emacs program.</td>\n",
       "      <td id=\"T_93f62_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_93f62_row3_col3\" class=\"data row3 col3\" >The output is completely faithful to the context provided. The context clearly states that Paris is the capital of France, and the output accurately reflects this information without adding any extraneous details or making any incorrect claims.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x106b7bf10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df = results.tables[\"eval_results_table\"]\n",
    "\n",
    "# just show question outputs faithfulness/v1/score\tfaithfulness/v1/justification columns\n",
    "df[[\"question\", \"outputs\", \"faithfulness/v1/score\", \"faithfulness/v1/justification\"]].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
