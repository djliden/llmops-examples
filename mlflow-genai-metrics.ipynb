{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Evaluation Metrics in MLflow\n",
    "\n",
    "MLflow 2.8 introduced new [Generative AI Metrics](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#generative-ai-metrics) that use LLMs to evaluate model output text. There are a few different GenAI metrics to choose from:\n",
    "- [Answer Correctness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which compares a model's output to a ground truth answer\n",
    "- [Answer Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness), which evaluates how appropriate and applicable a response is with respect to the input\n",
    "- [Answer Similarity](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity), which assesses the semantic similarity of a generated response to a ground truth answer\n",
    "- [Faithfulness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness), which tests the factual similarity of a model's response to some provided context (e.g. in a RAG system)\n",
    "- [Relevance](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.relevance), which examines the output with respect to the input and provided context (e.g. in a RAG system) and rates its relevance and significance. Note that this differs from the `Answer Similarity` metric, which does not have a context component.\n",
    "\n",
    "These all work in fudamentally the same way: pick a model and (optionally) define an example, at which point you can use the new metric in the MLflow.evaluate() system. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_relevance, greater_is_better=True, long_name=answer_relevance, version=v1, metric_details=\n",
      "Task:\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_relevance based on the input and output.\n",
      "A definition of answer_relevance and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer relevance measures the appropriateness and applicability of the output with respect to the input. Scores should reflect the extent to which the output directly addresses the question provided in the input, and give lower scores for incomplete or redundant output.\n",
      "\n",
      "Grading rubric:\n",
      "Answer relevance: Please give a score from 1-5 based on the degree of relevance to the input, where the lowest and highest scores are defined as follows:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the input.\n",
      "- Score 5: The output addresses all aspects of the question and all parts of the output are meaningful and relevant to the question.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input:\n",
      "What is MLflow Tracking?\n",
      "\n",
      "Output:\n",
      "MLflow Tracking provides both an API and UI dedicated to the logging of parameters, code versions, metrics, and artifacts during the ML process. This centralized repository captures details such as parameters, metrics, artifacts, data, and environment configurations, giving teams insight into their models’ evolution over time.\n",
      "\n",
      "\n",
      "\n",
      "score: 5\n",
      "justification: The answer directly addresses the input question and  provides a concise and clear description of MLflow Tracking.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is MLflow Model Registry?\n",
      "\n",
      "Output:\n",
      "MLflow Model Registry is a component of MLflow that helps in managing and deploying models in production. It provides versioning and stage transitions. MLflow also has a model evaluation feature for evaluating ML models.\n",
      "\n",
      "\n",
      "\n",
      "score: 3\n",
      "justification: The answer provides a general idea about MLflow Model Registry and includes correct details about versioning and stage transitions. The mention of model evaluation is irrelevant to the input question, hence the score of 3.\n",
      "        \n",
      "\n",
      "Input:\n",
      "What is automatic logging in MLflow?\n",
      "\n",
      "Output:\n",
      "Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\n",
      "\n",
      "\n",
      "\n",
      "score: 1\n",
      "justification: The output is completely irrelevant to the input question about automatic logging in MLflow, hence the score of 1.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response one below the other:\n",
      "score: Your numerical score for the model's answer_relevance based on the rubric\n",
      "justification: Your step-by-step reasoning about the model's answer_relevance score\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, answer_relevance\n",
    "\n",
    "example1 = EvaluationExample(\n",
    "    input=\"What is MLflow Tracking?\",\n",
    "    output=\"MLflow Tracking provides both an API and UI dedicated to the logging \"\n",
    "    \"of parameters, code versions, metrics, and artifacts during the ML process. \"\n",
    "    \"This centralized repository captures details such as parameters, metrics, \"\n",
    "    \"artifacts, data, and environment configurations, giving teams insight into their \"\n",
    "    \"models’ evolution over time.\",\n",
    "    score=5,\n",
    "    justification=\"The answer directly addresses the input question and  \"\n",
    "    \"provides a concise and clear description of MLflow Tracking.\",\n",
    ")\n",
    "\n",
    "example2 = EvaluationExample(\n",
    "    input=\"What is MLflow Model Registry?\",\n",
    "    output=\"MLflow Model Registry is a component of MLflow that helps in managing \"\n",
    "    \"and deploying models in production. It provides versioning and stage transitions. \"\n",
    "    \"MLflow also has a model evaluation feature for evaluating ML models.\",\n",
    "    score=3,\n",
    "    justification=\"The answer provides a general idea about MLflow Model Registry and \"\n",
    "    \"includes correct details about versioning and stage transitions. The mention of model evaluation \"\n",
    "    \"is irrelevant to the input question, hence the score of 3.\",\n",
    ")\n",
    "\n",
    "example3 = EvaluationExample(\n",
    "    input=\"What is automatic logging in MLflow?\",\n",
    "    output=\"Delta Lake is an open-source storage layer that brings ACID transactions to Apache \"\n",
    "    \"Spark and big data workloads.\",\n",
    "    score=1,\n",
    "    justification=\"The output is completely irrelevant to the input question about \"\n",
    "    \"automatic logging in MLflow, hence the score of 1.\",\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_relevance_metric = answer_relevance(model=\"openai:/gpt-4\", examples=[example1, example2, example3])\n",
    "\n",
    "print(answer_relevance_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Delta Lake?\",\n",
    "            \"How to exit vim?\",\n",
    "            \"How to exit emacs?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open source platform for managing the end-to-end machine learning lifecycle.\",\n",
    "            \"Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\",\n",
    "            \"To exit vim, press ESC to enter command mode, then type :q and press Enter.\",\n",
    "            \"To exit emacs, press Ctrl+x, then Ctrl+c.\"\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/30 17:27:25 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/10/30 17:27:25 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2023/10/30 17:27:31 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2023/10/30 17:27:31 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2023/10/30 17:27:31 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2023/10/30 17:27:31 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da3675540e44b9bad8e32bbed8bba79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/30 17:27:36 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/10/30 17:27:36 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/10/30 17:27:36 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2023/10/30 17:27:36 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/10/30 17:27:36 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2023/10/30 17:27:36 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/10/30 17:27:36 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n",
      "2023/10/30 17:27:36 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n",
      "2023/10/30 17:27:36 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_relevance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327a50057c844fbf88e96763507e80f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevance/v1/mean': 5.0,\n",
       " 'answer_relevance/v1/variance': 0.0,\n",
       " 'answer_relevance/v1/p90': 5.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Concisely answer the following question.\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        task=openai.ChatCompletion,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[answer_relevance_metric],  # use the answer similarity metric created above\n",
    "\n",
    "    )\n",
    "\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0b7410b1564e1d9d68b76dcafa434a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>answer_relevance/v1/score</th>\n",
       "      <th>answer_relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open source platform for managing...</td>\n",
       "      <td>MLflow is an open-source platform that assists...</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly addresses the input questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Delta Lake?</td>\n",
       "      <td>Delta Lake is an open-source storage layer tha...</td>\n",
       "      <td>Delta Lake is an open-source data lake storage...</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provides a comprehensive and accura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to exit vim?</td>\n",
       "      <td>To exit vim, press ESC to enter command mode, ...</td>\n",
       "      <td>To exit Vim, you can press the Esc key to swit...</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly answers the input question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to exit emacs?</td>\n",
       "      <td>To exit emacs, press Ctrl+x, then Ctrl+c.</td>\n",
       "      <td>To exit Emacs, you can use the keyboard shortc...</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly addresses the input questi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                inputs                                       ground_truth  \\\n",
       "0      What is MLflow?  MLflow is an open source platform for managing...   \n",
       "1  What is Delta Lake?  Delta Lake is an open-source storage layer tha...   \n",
       "2     How to exit vim?  To exit vim, press ESC to enter command mode, ...   \n",
       "3   How to exit emacs?          To exit emacs, press Ctrl+x, then Ctrl+c.   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  MLflow is an open-source platform that assists...           22   \n",
       "1  Delta Lake is an open-source data lake storage...           49   \n",
       "2  To exit Vim, you can press the Esc key to swit...           26   \n",
       "3  To exit Emacs, you can use the keyboard shortc...           41   \n",
       "\n",
       "   answer_relevance/v1/score  \\\n",
       "0                          5   \n",
       "1                          5   \n",
       "2                          5   \n",
       "3                          5   \n",
       "\n",
       "                   answer_relevance/v1/justification  \n",
       "0  The output directly addresses the input questi...  \n",
       "1  The output provides a comprehensive and accura...  \n",
       "2  The output directly answers the input question...  \n",
       "3  The output directly addresses the input questi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x289cc2630> JSON: {\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"role\": \"assistant\",\n",
       "    \"content\": \"Hello! How can I assist you today?\"\n",
       "  },\n",
       "  \"finish_reason\": \"stop\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "prompt = \"test openai chat completion @OpenAI\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "completion = response.choices[0]\n",
    "\n",
    "completion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
