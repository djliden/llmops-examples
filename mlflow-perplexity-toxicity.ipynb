{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe87eb8-5e7d-4301-b1cb-e47045889181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Evaluate LLMs on Perplexity and Toxicity\n",
    "\n",
    "MLflow now offers the option to evaluate LLMs on Perplexity and Toxicity. This notebook will briefly guide you through the process of using `mlflow.evaluate` to compare perplexity and toxicity.\n",
    "\n",
    "To get started with evaluation, check out [this notebook](./compare-llms-with-mlflow.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739b3e51-3e95-4f9f-b4d6-9394f7adeab7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Setup\n",
    "We will compare OpenAI's `gpt-3.5-turbo` and Meta's `Llama-2-70b-chat` (via the Mosaic inference starter tier) using MLflow AI Gateway. But you can use any models compatible with the MLflow Evaluation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85823c1b-4278-4b17-a29d-935a95004269",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade 'mlflow[gateway]'\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840855e5-3a89-4e83-a353-aaeff925033a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# configure keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mlflow.gateway\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = dbutils.secrets.get(scope=\"daniel.liden\", key=\"OPENAI_API_KEY\")\n",
    "MOSAIC_API_KEY = dbutils.secrets.get(scope=\"daniel.liden\", key=\"MOSAIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e1090f-8e14-4fd4-a804-c86c2000c0b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only necessary if you haven't already created the models\n",
    "\n",
    "# mlflow.gateway.delete_route(\"dl-gpt-3_5-turbo\")\n",
    "mlflow.gateway.create_route(\n",
    "    name=\"dl-gpt-3_5-turbo\",\n",
    "    route_type=\"llm/v1/chat\",\n",
    "    model= {\n",
    "        \"name\": \"gpt-3.5-turbo\", \n",
    "        \"provider\": \"openai\",\n",
    "        \"openai_config\": {\n",
    "          \"openai_api_key\": OPENAI_API_KEY,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# mlflow.gateway.delete_route(\"dl-llama-70b-chat-mosaic\")\n",
    "mlflow.gateway.create_route(\n",
    "    name=\"dl-llama-70b-chat-mosaic\",\n",
    "    route_type=\"llm/v1/chat\",\n",
    "    model= {\n",
    "        \"name\": \"llama2-70b-chat\", \n",
    "        \"provider\": \"mosaicml\",\n",
    "        \"mosaicml_config\": {\n",
    "          \"mosaicml_api_key\": MOSAIC_API_KEY,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "683cae19-31ce-4f47-86e6-7a31b8775ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Register MLflow AI Gateway models\n",
    "\n",
    "Now register the models with AI gateway, storing the run IDs so we can evaluate in the same runs.\n",
    "\n",
    "Note: `MLflow.evaluate` evaluation datasets can't have dictionaries within columns, which is what the chat models expect as inputs. Later, we will convert input dictionaries to JSON strings. So the predict methods defined below include logic for converting JSON strings back to dictionaries when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5017cb9b-03a2-4930-9c39-7383f6f5f6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.gateway import MlflowGatewayClient\n",
    "\n",
    "\n",
    "# configure the predict methods for the custom models\n",
    "def predict_mosaic(data):\n",
    "    client = MlflowGatewayClient(\"databricks\")\n",
    "    # Convert JSON strings in 'messages' column to dicts if necessary\n",
    "    data[\"messages\"] = data[\"messages\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    payload = data.to_dict(orient=\"records\")\n",
    "    return [\n",
    "        client.query(route=\"dl-llama-70b-chat-mosaic\",\n",
    "                     data=query)['candidates'][0]['message']['content']\n",
    "        for query in payload\n",
    "    ]\n",
    "\n",
    "def predict_openai(data):\n",
    "    client = MlflowGatewayClient(\"databricks\")\n",
    "    # Convert JSON strings in 'messages' column to dicts if necessary\n",
    "    data[\"messages\"] = data[\"messages\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    payload = data.to_dict(orient=\"records\")\n",
    "    return [\n",
    "        client.query(route=\"dl-gpt-3_5-turbo\",\n",
    "                     data=query)['candidates'][0]['message']['content']\n",
    "        for query in payload\n",
    "    ]\n",
    "\n",
    "\n",
    "# generate input examples\n",
    "input_example = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"Very concisely explain MLflow runs.\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"No.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Very concisely explain MLflow runs.\"},\n",
    "            ],\n",
    "            [{\"role\": \"user\", \"content\": \"Very concisely explain MLflow artifacts.\"}],\n",
    "        ],\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 50,\n",
    "    }\n",
    ")\n",
    "\n",
    "# generate model signatures\n",
    "signature = mlflow.models.infer_signature(\n",
    "    input_example, [\"MLflow runs are...\", \"MLflow artifacts are...\"]\n",
    ")\n",
    "\n",
    "# record run ids and log models\n",
    "run_ids = []\n",
    "\n",
    "with mlflow.start_run(run_name=\"log_mosaic_model\"):\n",
    "    run_ids.append(mlflow.active_run().info.run_id)\n",
    "    mosaic_model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=predict_mosaic,\n",
    "        registered_model_name=\"dl-llama-70b-chat-mosaic-gateway\",\n",
    "        artifact_path=\"dbfs/daniel.liden/mlflow/dl-llama-70b-chat-mosaic-gateway/\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "with mlflow.start_run(run_name=\"log_openai_model\"):\n",
    "    run_ids.append(mlflow.active_run().info.run_id)\n",
    "    openai_model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=predict_openai,\n",
    "        registered_model_name=\"dl-gpt-3_5-turbo-gateway\",\n",
    "        artifact_path=\"dbfs/daniel.liden/mlflow/dl-gpt-3_5-turbo-gateway/\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "# optionally load the models\n",
    "loaded_mosaic_model = mlflow.pyfunc.load_model(mosaic_model_info.model_uri)\n",
    "loaded_openai_model = mlflow.pyfunc.load_model(openai_model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f6bd594-886d-4fce-919c-07f53f68f161",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Define an evaluation dataset\n",
    "\n",
    "As noted above, we convert the input dicts into JSON strings as required by `mlflow.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35074fe7-c0f5-49dd-86fb-bce001ec8ebe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "        \"messages\": [\n",
    "            [{\"role\": \"user\", \"content\": \"Very concisely explain MLflow runs.\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"Very concisely explain MLflow artifacts.\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"How does backpropagation work?\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"Differences between RNN and LSTM?\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"What is transfer learning?\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"Explain the concept of overfitting.\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"How are CNNs used in image recognition?\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"What does the term 'bias' mean in AI?\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"Describe reinforcement learning briefly.\"}],\n",
    "            [{\"role\": \"user\", \"content\": \"Tell me about gradient descent.\"}],\n",
    "        ],\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 50,\n",
    "    }\n",
    "\n",
    "data[\"messages\"] = [json.dumps(item) for item in data[\"messages\"]]\n",
    "eval_df = pd.DataFrame.from_dict(data)\n",
    "eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adaeeec2-87a3-4fed-9fbe-f4f185afb961",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Evaluate with perplexity and toxicity\n",
    "perplexity and toxicity are now default evaluation metrics for models with `model_type=\"text\"`, so we don't need to do anything special to add these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b92f8a-af7c-458e-9835-1ed5abc5005d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id = run_ids[0]):\n",
    "    eval_df[\"model_name\"] = \"mosaic_llama\"\n",
    "    mlflow.evaluate(\n",
    "        #model=loaded_mosaic_model,\n",
    "        model = mosaic_model_info.model_uri,\n",
    "        data=eval_df,\n",
    "        model_type=\"text\",\n",
    "    )\n",
    "\n",
    "with mlflow.start_run(run_id = run_ids[1]):\n",
    "    eval_df[\"model_name\"] = \"openai\"\n",
    "    mlflow.evaluate(\n",
    "        #model=loaded_openai_model,\n",
    "        model = openai_model_info.model_uri,\n",
    "        data=eval_df,\n",
    "        model_type=\"text\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d29a154-a868-4121-b62f-b5a36c6ff6f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can view the results in the MLflow UI or load the evaluation table as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba48bef0-ef44-48a5-b883-ebdf75a2af79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.load_table(\"eval_results_table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8542ef9f-748a-41f3-9284-f712647d0ceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "mlflow-perplexity-toxicity",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
