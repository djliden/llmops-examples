{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Prompt Engineering UI\n",
    "This notebook demonstrates how to use the new MLflow Prompt Engineering UI to compare the Mosaic Inferencer Starter Tier Llama v2 70b chat model to GPT 3.5 Turbo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "MOSAIC_API_KEY = os.environ[\"MOSAIC_API_KEY\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenAI and Mosaic Gateway Routes\n",
    "See [config.yaml](mlflow_prompt_eng_ui_assets/config.yaml) for the configuration. It looks like this:\n",
    "\n",
    "```yaml\n",
    "routes: \n",
    "  - name: chat_openai\n",
    "    route_type: llm/v1/chat\n",
    "    model:\n",
    "      provider: openai\n",
    "      name: gpt-3.5-turbo\n",
    "      config:\n",
    "        openai_api_key: $OPENAI_API_KEY\n",
    "  - name: chat_mosaic_llama\n",
    "    route_type: llm/v1/chat\n",
    "    model:\n",
    "      provider: mosaic\n",
    "      name: llama2-70b-chat\n",
    "      config:\n",
    "        mosaic_api_key: $MOSAIC_API_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.gateway import MlflowGatewayClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mlflow gateway start --config-path mlflow_prompt_eng_ui_assets/config.yaml --port 5000 --host localhost --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run that as a subprocess\n",
    "import subprocess\n",
    "\n",
    "cmd = [\"mlflow\", \"gateway\", \"start\", \"--config-path\", \"config.yaml\", \"--port\", \"5000\", \"--host\", \"localhost\", \"--workers\", \"2\"]\n",
    "process = subprocess.Popen(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if you need to terminate the process\n",
    "#process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gateway_client = MlflowGatewayClient(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway_client.search_routes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Test Query against each route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the openai route\n",
    "response = gateway_client.query(\n",
    "    \"chat_openai\", {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about rabbits\"}]}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gateway_client.query(\n",
    "    \"chat_mosaic_llama\", {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about rabbits\"}]}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in MLflow Prompt Engineering UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, run the following to tell MLflow where to find the gateway routes.\n",
    "\n",
    "`export MLFLOW_GATEWAY_URI=\"http://localhost:5000\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new mlflow experiment\n",
    "mlflow.set_experiment(\"mlflow_ui_example\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the mlflow ui with:\n",
    "`mlflow ui --port 5001`\n",
    "\n",
    "(It is only necessary to specify the port if you have something else running on port 5000; in this case, I started the AI gateway on port 5000).\n",
    "\n",
    "We will do everything else in the UI!\n",
    "\n",
    "## Start a new run with the prompt UI:\n",
    "\n",
    "![](./mlflow_prompt_eng_ui_assets/screenshots/1.png)\n",
    "\n",
    "## Select a model and configure a prompt template\n",
    "\n",
    "![](./mlflow_prompt_eng_ui_assets/screenshots/2.png)\n",
    "\n",
    "## Add and compare more models and more prompt inputs\n",
    "\n",
    "![](./mlflow_prompt_eng_ui_assets/screenshots/3.png)\n",
    "\n",
    "![](./mlflow_prompt_eng_ui_assets/screenshots/4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [MLflow AI Gateway Docs](https://mlflow.org/docs/latest/gateway/index.html)\n",
    "- [MLflow Prompt Engineering UI Docs](https://mlflow.org/docs/latest/llms/prompt-engineering.html#)\n",
    "- [MosaicML Starter Tier Docs](https://docs.mosaicml.com/en/latest/inference.html#starter-tier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
