{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "MOSAIC_API_KEY = os.environ[\"MOSAIC_API_KEY\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenAI and Mosaic Gateway Routes\n",
    "See [config.yaml](./config.yaml) for the configuration. It looks like this:\n",
    "\n",
    "```yaml\n",
    "routes: \n",
    "  - name: chat_openai\n",
    "    route_type: llm/v1/chat\n",
    "    model:\n",
    "      provider: openai\n",
    "      name: gpt-3.5-turbo\n",
    "      config:\n",
    "        openai_api_key: $OPENAI_API_KEY\n",
    "  - name: chat_mosaic_llama\n",
    "    route_type: llm/v1/chat\n",
    "    model:\n",
    "      provider: mosaic\n",
    "      name: llama2-70b-chat\n",
    "      config:\n",
    "        mosaic_api_key: $MOSAIC_API_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.gateway import MlflowGatewayClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mlflow gateway start --config-path config.yaml --port 5000 --host localhost --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "[2023-09-15 12:12:11 -0400] [16893] [INFO] Starting gunicorn 21.2.0\n",
      "[2023-09-15 12:12:11 -0400] [16893] [INFO] Listening at: http://127.0.0.1:5000 (16893)\n",
      "[2023-09-15 12:12:11 -0400] [16893] [INFO] Using worker: uvicorn.workers.UvicornWorker\n",
      "[2023-09-15 12:12:11 -0400] [16894] [INFO] Booting worker with pid: 16894\n",
      "[2023-09-15 12:12:11 -0400] [16895] [INFO] Booting worker with pid: 16895\n",
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/daniel.liden/projects/prompt_ui/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "[2023-09-15 12:12:13 -0400] [16895] [INFO] Started server process [16895]\n",
      "[2023-09-15 12:12:13 -0400] [16894] [INFO] Started server process [16894]\n",
      "[2023-09-15 12:12:13 -0400] [16895] [INFO] Waiting for application startup.\n",
      "[2023-09-15 12:12:13 -0400] [16894] [INFO] Waiting for application startup.\n",
      "[2023-09-15 12:12:13 -0400] [16895] [INFO] Application startup complete.\n",
      "[2023-09-15 12:12:13 -0400] [16894] [INFO] Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "# let's run that as a subprocess\n",
    "import subprocess\n",
    "\n",
    "cmd = [\"mlflow\", \"gateway\", \"start\", \"--config-path\", \"config.yaml\", \"--port\", \"5000\", \"--host\", \"localhost\", \"--workers\", \"2\"]\n",
    "process = subprocess.Popen(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if you need to terminate the process\n",
    "#process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gateway_client = MlflowGatewayClient(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Route(name='chat_openai', route_type='llm/v1/chat', model=RouteModelInfo(name='gpt-3.5-turbo', provider='openai'), route_url='http://localhost:5000/gateway/chat_openai/invocations'),\n",
       " Route(name='chat_mosaic_llama', route_type='llm/v1/chat', model=RouteModelInfo(name='llama2-70b-chat', provider='mosaicml'), route_url='http://localhost:5000/gateway/chat_mosaic_llama/invocations')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gateway_client.search_routes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Test Query against each route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidates': [{'message': {'role': 'assistant', 'content': 'Why did the rabbit go to the barber?\\n\\nBecause it wanted a hare-cut!'}, 'metadata': {'finish_reason': 'stop'}}], 'metadata': {'input_tokens': 13, 'output_tokens': 16, 'total_tokens': 29, 'model': 'gpt-3.5-turbo-0613', 'route_type': 'llm/v1/chat'}}\n"
     ]
    }
   ],
   "source": [
    "# test the openai route\n",
    "response = gateway_client.query(\n",
    "    \"chat_openai\", {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about rabbits\"}]}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidates': [{'message': {'role': 'assistant', 'content': ' Sure, here\\'s a classic one:\\n\\nWhy did the rabbit go to the doctor?\\n\\nBecause he had hare-loss!\\n\\n(This joke plays on the word \"hare,\" which is a synonym for rabbit, and \"hair loss,\" which is a common medical condition in humans.)\\n\\nI hope that made you hop with laughter!'}, 'metadata': {'finish_reason': None}}], 'metadata': {'input_tokens': None, 'output_tokens': None, 'total_tokens': None, 'model': 'llama2-70b-chat', 'route_type': 'llm/v1/chat'}}\n"
     ]
    }
   ],
   "source": [
    "response = gateway_client.query(\n",
    "    \"chat_mosaic_llama\", {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about rabbits\"}]}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in MLflow Prompt Engineering UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, run the following to tell MLflow where to find the gateway routes.\n",
    "\n",
    "`export MLFLOW_GATEWAY_URI=\"http://localhost:5000\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/15 12:28:22 INFO mlflow.tracking.fluent: Experiment with name 'mlflow_ui_example' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/daniel.liden/projects/prompt_ui/mlruns/749607414286048058', creation_time=1694795302669, experiment_id='749607414286048058', last_update_time=1694795302669, lifecycle_stage='active', name='mlflow_ui_example', tags={}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new mlflow experiment\n",
    "mlflow.set_experiment(\"mlflow_ui_example\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the mlflow ui with:\n",
    "`mlflow ui --port 5001`\n",
    "\n",
    "(It is only necessary to specify the port if you have something else running on port 5000; in this case, I started the AI gateway on port 5000).\n",
    "\n",
    "We will do everything else in the UI!\n",
    "\n",
    "## Start a new run with the prompt UI:\n",
    "\n",
    "![](./screenshots/1.png)\n",
    "\n",
    "## Select a model and configure a prompt template\n",
    "\n",
    "![](./screenshots/2.png)\n",
    "\n",
    "## Add and compare more models and more prompt inputs\n",
    "\n",
    "![](./screenshots/3.png)\n",
    "\n",
    "![](./screenshots/4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
