{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Chat Models with MLflow\n",
    "\n",
    "## Resources\n",
    "- [MLflow PyFunc Chat Model Tutorial](https://mlflow.org/docs/latest/llms/transformers/tutorials/conversational/pyfunc-chat-model.html)\n",
    "- [MLflow PyFunc ChatModel API](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ChatModel)\n",
    "\n",
    "## Outline\n",
    "- What makes a chat model different\n",
    "- Why defining a Chat Model with Pyfunc is kind of hard\n",
    "- ChatModel class makes it easier\n",
    "  - Show equivalent ChatModel and PyFunc model definition\n",
    "- Add-on: retriever in ChatModel\n",
    "- Add-on: streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models with MLflow\n",
    "\n",
    "OpenAI's model input and output schemas have become the defacto standards among LLM providers. Compatibility with the OpenAI API spec enables models to integrate seamlessly with many different AI tools, evaluation systems, UIs, and more.\n",
    "\n",
    "This post shows how to make [custom PyFunc models](https://mlflow.org/blog/custom-pyfunc) that conform to the OpenAI API spec.\n",
    "\n",
    "## The ChatModel Class\n",
    "\n",
    "As of MLflow 2.11, you can use the `ChatModel` class to define custom chat models. `ChatModel` is a subclass of `PythonModel` that automatically defines input/output signatures that are compatible with the OpenAI API spec. Let's try a simple example and wrap Google's Gemma 2B model with `ChatModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gemma = pipeline(\"text-generation\", model=\"google/gemma-2b-it\",\n",
    "                 device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"ChatModel\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        artifact_path=\"gemma-text-generation\", transformers_model=gemma,\n",
    "        task=\"llm/v1/chat\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the previously saved MLflow model\n",
    "model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Tell me a short joke about AI.\"}]\n",
    "model.predict({\"messages\": messages, \"max_tokens\": 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_stream({\"messages\": messages, \"max_tokens\": 25})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to customize our model. For example, let's implement the predict_stream method so our model can stream responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Generator, List\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ChatResponse, ChatMessage\n",
    "\n",
    "class GemmaChatModel(mlflow.pyfunc.ChatModel):\n",
    "    def load_context(self, context):\n",
    "        # load our previously-saved Transformers pipeline from context.artifacts\n",
    "        self.pipeline = mlflow.transformers.load_model(context.artifacts[\"chat_model_path\"])\n",
    "\n",
    "    def preprocess_messages(self, messages: List[ChatMessage]) -> List[ChatMessage]:\n",
    "        preprocessed = []\n",
    "        for i, message in enumerate(messages):\n",
    "            if message.role == \"system\":\n",
    "                preprocessed.append(ChatMessage(role=\"user\", content=message.content))\n",
    "            else:\n",
    "                preprocessed.append(message)\n",
    "            \n",
    "            # If we just added a user message and it's not the last one, add a blank assistant message\n",
    "            if message.role in [\"user\", \"system\"] and i < len(messages) - 1:\n",
    "                preprocessed.append(ChatMessage(role=\"assistant\", content=\" \"))\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "    def predict(self, context, messages, params):\n",
    "        tokenizer = self.pipeline.tokenizer\n",
    "        preprocessed_messages = self.preprocess_messages(messages)\n",
    "        prompt = tokenizer.apply_chat_template(preprocessed_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # perform inference using the loaded pipeline\n",
    "        output = self.pipeline(prompt, return_full_text=False, generation_kwargs=params.to_dict(), max_new_tokens=100)\n",
    "        text = output[0][\"generated_text\"]\n",
    "        id = str(uuid.uuid4())\n",
    "\n",
    "        # construct token usage information\n",
    "        prompt_tokens = len(tokenizer.encode(prompt))\n",
    "        completion_tokens = len(tokenizer.encode(text))\n",
    "        usage = {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": prompt_tokens + completion_tokens,\n",
    "        }\n",
    "\n",
    "        response = {\n",
    "            \"id\": id,\n",
    "            \"model\": \"MyChatModel\",\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"message\": {\"role\": \"assistant\", \"content\": text},\n",
    "                    \"finish_reason\": \"stop\",\n",
    "                }\n",
    "            ],\n",
    "            \"usage\": usage,\n",
    "        }\n",
    "\n",
    "        return ChatResponse(**response)\n",
    "\n",
    "    def predict_stream(self, context, messages, params) -> Generator[ChatResponse, None, None]:\n",
    "        tokenizer = self.pipeline.tokenizer\n",
    "        preprocessed_messages = self.preprocess_messages(messages)\n",
    "        prompt = tokenizer.apply_chat_template(preprocessed_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        id = str(uuid.uuid4())\n",
    "        prompt_tokens = len(tokenizer.encode(prompt))\n",
    "        accumulated_text = \"\"\n",
    "\n",
    "        # perform streaming inference using the loaded pipeline\n",
    "        for output in self.pipeline(prompt, return_full_text=False, generation_kwargs=params.to_dict(), streaming=True):\n",
    "            new_text = output[\"generated_text\"]\n",
    "            accumulated_text += new_text\n",
    "\n",
    "            # construct token usage information for this chunk\n",
    "            completion_tokens = len(tokenizer.encode(new_text))\n",
    "            usage = {\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"total_tokens\": prompt_tokens + completion_tokens,\n",
    "            }\n",
    "\n",
    "            response = {\n",
    "                \"id\": id,\n",
    "                \"model\": \"MyChatModel\",\n",
    "                \"choices\": [\n",
    "                    {\n",
    "                        \"index\": 0,\n",
    "                        \"delta\": {\"role\": \"assistant\", \"content\": new_text},\n",
    "                        \"finish_reason\": None,\n",
    "                    }\n",
    "                ],\n",
    "                \"usage\": usage,\n",
    "            }\n",
    "\n",
    "            yield ChatResponse(**response)\n",
    "\n",
    "        # Final yield with finish_reason \"stop\"\n",
    "        final_response = {\n",
    "            \"id\": id,\n",
    "            \"model\": \"MyChatModel\",\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"delta\": {\"role\": \"assistant\", \"content\": \"\"},\n",
    "                    \"finish_reason\": \"stop\",\n",
    "                }\n",
    "            ],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": len(tokenizer.encode(accumulated_text)),\n",
    "                \"total_tokens\": prompt_tokens + len(tokenizer.encode(accumulated_text)),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield ChatResponse(**final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"ChatModel\")\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    custom_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"gemma-text-generation-custom\", python_model=GemmaChatModel(),\n",
    "        artifacts = {\"chat_model_path\": \"file:///Users/daniel.liden/git/llmops-examples/mlruns/387827672116713370/410c3b2e8b3747348c73f15b48a5b82d/artifacts/gemma-text-generation\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model = mlflow.pyfunc.load_model(custom_model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Tell me a short joke about AI.\"}]\n",
    "model.predict({\"messages\": messages, \"max_tokens\": 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Prepare messages\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful AI assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a short story about a brave adventurer, one sentence at a time.\")\n",
    "]\n",
    "\n",
    "# Set up parameters\n",
    "params = {\n",
    "    \"max_length\": 1000,  # Adjust as needed\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "# Use predict_stream\n",
    "print(\"Generating story:\")\n",
    "for response in model.predict_stream(messages, params):\n",
    "    chunk = response.choices[0].delta.content\n",
    "    print(chunk, end='', flush=True)\n",
    "    sys.stdout.flush()  # Ensure output is displayed immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
